{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abdba210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "#%matplotlib inline\n",
    "import math, json\n",
    "import warnings\n",
    "#import intake\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import defaultdict\n",
    "#import matplotlib\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "from scipy import stats\n",
    "#import cftime\n",
    "import xarray as xr\n",
    "xr.set_options(display_style='html')\n",
    "import s3fs\n",
    "#import spei\n",
    "\n",
    "import datetime, calendar\n",
    "\n",
    "#plt.style.use(\"seaborn-darkgrid\")\n",
    "import coiled\n",
    "#ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6ecf022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pynative.com/python-serialize-numpy-ndarray-into-json/\n",
    "from json import JSONEncoder\n",
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28f44297",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIST_START = 1980\n",
    "HIST_END = 2014\n",
    "\n",
    "FUTURE_START = 2080\n",
    "FUTURE_END = 2100\n",
    "\n",
    "PERCENTILE_STARTYEAR = 1980\n",
    "PERCENTILE_ENDYEAR = 2019\n",
    "\n",
    "NUM_BEST_MODELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "747d9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CITYLATLON = {}\n",
    "with open('ghsl_500k.csv', 'r') as ifile:\n",
    "    for line in ifile.readlines():\n",
    "        items = [i.strip() for i in line.split(',')]\n",
    "        CITYLATLON['city_{0}'.format(items[0])] = (float(items[2]), float(items[3]), int(items[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24b29c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('citiesonly.csv', 'r', encoding='utf-8') as ifile:\n",
    "    CITYLATLON = {i[0]: (float(i[1]), float(i[2]), i[3]) for i in [j.split(',') + [idx] for idx, j in enumerate(ifile.readlines()[1:])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d04f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    'tasmax': ('GFDL-ESM4', 'CanESM5', 'MRI-ESM2-0', 'IPSL-CM6A-LR', 'EC-Earth3-Veg-LR'),\n",
    "    'tasmin': ('GFDL-ESM4', 'IPSL-CM6A-LR', 'CanESM5', 'MRI-ESM2-0', 'EC-Earth3-Veg-LR'),\n",
    "    'pr': ('GFDL-ESM4', 'IPSL-CM6A-LR', 'CanESM5', 'EC-Earth3-Veg-LR'),\n",
    "    'hurs': ('GFDL-ESM4', 'CanESM5', 'MRI-ESM2-0', 'IPSL-CM6A-LR', 'EC-Earth3-Veg-LR'),\n",
    "    'sfcWind': ('GFDL-ESM4', 'CanESM5', 'IPSL-CM6A-LR')\n",
    "}\n",
    "\n",
    "MODELRUN = 'r1i1p1f1'\n",
    "\n",
    "MODELGRID = {\n",
    "    'GFDL-ESM4': 'gr1',\n",
    "    'CanESM5': 'gn',\n",
    "    'MRI-ESM2-0': 'gn',\n",
    "    'IPSL-CM6A-LR': 'gr',\n",
    "    'EC-Earth3-Veg-LR': 'gr'\n",
    "}\n",
    "\n",
    "YEARLENGTH = {\n",
    "    'GFDL-ESM4': 365,\n",
    "    'CanESM5': 365,\n",
    "    'MRI-ESM2-0': 366,\n",
    "    'IPSL-CM6A-LR': 366,\n",
    "    'EC-Earth3-Veg-LR': 366\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f32606e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FAMILY = {'UKESM1-0-LL': 'HadAM',\n",
    " 'NorESM2-MM': 'CCM',\n",
    " 'NorESM2-LM': 'CCM',\n",
    " 'MRI-ESM2-0': 'UCLA GCM',\n",
    " 'MPI-ESM1-2-LR': 'ECMWF',\n",
    " 'MPI-ESM1-2-HR': 'ECMWF',\n",
    " 'MIROC6': 'MIROC',\n",
    " 'MIROC-ES2L': 'MIROC',\n",
    " 'KIOST-ESM': 'GFDL',\n",
    " 'KACE-1-0-G': 'HadAM',\n",
    " 'IPSL-CM6A-LR': 'IPSL',\n",
    " 'INM-CM5-0': 'INM',\n",
    " 'INM-CM4-8': 'INM',\n",
    " 'HadGEM3-GC31-MM': 'HadAM',\n",
    " 'HadGEM3-GC31-LL': 'HadAM',\n",
    " 'GFDL-ESM4': 'GFDL',\n",
    " 'GFDL-CM4_gr2': 'GFDL',\n",
    " 'GFDL-CM4': 'GFDL',\n",
    " 'FGOALS-g3': 'CCM',\n",
    " 'EC-Earth3-Veg-LR': 'ECMWF',\n",
    " 'EC-Earth3': 'ECMWF',\n",
    " 'CanESM5': 'CanAM',\n",
    " 'CNRM-ESM2-1': 'ECMWF',\n",
    " 'CNRM-CM6-1': 'ECMWF',\n",
    " 'CMCC-ESM2': 'CCM',\n",
    " 'CMCC-CM2-SR5': 'CCM',\n",
    " 'BCC-CSM2-MR': 'CCM',\n",
    " 'ACCESS-ESM1-5': 'HadAM',\n",
    " 'ACCESS-CM2': 'HadAM',\n",
    " 'TaiESM1': 'CCM',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66e61532",
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = {\n",
    "    'tasmax': {\n",
    "        'era_varname': 'maximum_2m_air_temperature',\n",
    "        'nex_transform': lambda x: x - 273.5,\n",
    "        'era_transform': lambda x: x - 273.5\n",
    "    },\n",
    "    'tasmin': {\n",
    "        'era_varname': 'minimum_2m_air_temperature',\n",
    "        'nex_transform': lambda x: x - 273.5,\n",
    "        'era_transform': lambda x: x - 273.5\n",
    "    },\n",
    "\n",
    "    'pr': {\n",
    "        'era_varname': 'total_precipitation',\n",
    "        'nex_transform': lambda x: x * 86400,\n",
    "        'era_transform': lambda x: x * 1000\n",
    "    },\n",
    "    'hurs': {\n",
    "        'era_varname': None,\n",
    "        'nex_transform': lambda x: x,\n",
    "        'era_transform': lambda x: x\n",
    "    },\n",
    "    'sfcWind': {\n",
    "        'era_varname': None,\n",
    "        'nex_transform': lambda x: x * 3600 / 1000,\n",
    "        'era_transform': lambda x: x * 3600 / 1000\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1593bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnperiod_value_daily(nex_varname, rp, latlon):\n",
    "    era_varname = VARIABLES[nex_varname]['era_varname']\n",
    "    hist_start = PERCENTILE_STARTYEAR\n",
    "    hist_end = PERCENTILE_ENDYEAR\n",
    "    allyears = []\n",
    "    for year in range(PERCENTILE_STARTYEAR, PERCENTILE_ENDYEAR):\n",
    "        allyears.append(VARIABLES[nex_varname]['era_transform'](get_eravar(era_varname, latlon, start_year=year, end_year=year, southern_hem=False)))\n",
    "    d = np.sort(np.concatenate(allyears).flatten())\n",
    "    d = d[d > 0.01]  # Only consider actual positive events\n",
    "    vals, counts = np.unique(d, return_counts=True)\n",
    "    freqs = counts / d.size\n",
    "    cdf_y = np.cumsum(freqs)\n",
    "    targetfreq = (PERCENTILE_ENDYEAR - PERCENTILE_STARTYEAR + 1) / rp\n",
    "    return np.interp(1-targetfreq, vals, cdf_y)\n",
    "    \n",
    "\n",
    "def calendardate_percentiles(nex_varname, q, latlon, sh_hem=False):\n",
    "    era_varname = VARIABLES[nex_varname]['era_varname']\n",
    "    hist_start = PERCENTILE_STARTYEAR\n",
    "    hist_end = PERCENTILE_ENDYEAR\n",
    "    allyears = []\n",
    "    for year in range(PERCENTILE_STARTYEAR, PERCENTILE_ENDYEAR):\n",
    "        allyears.append(VARIABLES[nex_varname]['era_transform'](get_eravar(era_varname, latlon, start_year=year, end_year=year, southern_hem=False)))\n",
    "    if not sh_hem:\n",
    "        return np.percentile(np.vstack(allyears), q, axis=0)\n",
    "    else:\n",
    "        res = np.percentile(np.vstack(allyears), q, axis=0)\n",
    "        return np.concatenate([res[152:], res[:152]])\n",
    "\n",
    "def wholeyear_percentile(nex_varname, q, latlon):\n",
    "    era_varname = VARIABLES[nex_varname]['era_varname']\n",
    "    hist_start = PERCENTILE_STARTYEAR\n",
    "    hist_end = PERCENTILE_ENDYEAR\n",
    "    allyears = []\n",
    "    for year in range(hist_start, hist_end):\n",
    "        allyears.append(VARIABLES[nex_varname]['era_transform'](get_eravar(era_varname, latlon, start_year=year, end_year=year, southern_hem=False)))\n",
    "    return np.percentile(np.concatenate(allyears).flatten(), q)\n",
    "\n",
    "def yearextreme_percentile(nex_varname, q, latlon, wantmax):\n",
    "    era_varname = VARIABLES[nex_varname]['era_varname']\n",
    "    hist_start = PERCENTILE_STARTYEAR\n",
    "    hist_end = PERCENTILE_ENDYEAR\n",
    "    allyears = []\n",
    "    for year in range(hist_start, hist_end):\n",
    "        allyears.append([np.min, np.max][int(wantmax)](VARIABLES[nex_varname]['era_transform'](get_eravar(era_varname, latlon, start_year=year, end_year=year, southern_hem=False))))\n",
    "    return np.percentile(np.array(allyears), q)\n",
    "\n",
    "def thresholdexceedance_mediancount(nex_varname, threshold, latlon, want_gte):\n",
    "    era_varname = VARIABLES[nex_varname]['era_varname']\n",
    "    data = VARIABLES[nex_varname]['era_transform'](get_eravar(era_varname, latlon, start_year=PERCENTILE_STARTYEAR, end_year=PERCENTILE_ENDYEAR, southern_hem=False))\n",
    "    if data.size % 365 != 0:\n",
    "        raise Exception('Data array length is not an integer multiple of 365')\n",
    "    byyear = data.reshape(data.size//365, 365)\n",
    "    if want_gte:\n",
    "        return np.median(np.sum(byyear >= threshold, axis=1))\n",
    "    else:\n",
    "        return np.median(np.sum(byyear <= threshold, axis=1))\n",
    "\n",
    "\n",
    "def get_rmsd(d1, d2):\n",
    "    c1 = seasonal_means(d1)\n",
    "    c2 = seasonal_means(d2)\n",
    "    return np.sqrt(np.mean(np.sum((c1 - c2)**2)))\n",
    "\n",
    "def count_runs(tf_array, min_runsize):\n",
    "    falses = np.zeros(tf_array.shape[0]).reshape((tf_array.shape[0],1))\n",
    "    extended_a = np.concatenate([[0], tf_array, [0]])\n",
    "    df = np.diff(extended_a)\n",
    "    starts = np.nonzero(df == 1)[0]\n",
    "    ends = np.nonzero(df == -1)[0]\n",
    "    count = 0\n",
    "    for idx in range(starts.size):\n",
    "        if ends[idx] - starts[idx] >= min_runsize:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def removeLeapDays(arr, start_year, end_year, extralong=False, southern_hem=False):\n",
    "    if extralong:\n",
    "        indices = list(range(184))\n",
    "        jan1_idx = 184\n",
    "\n",
    "        for year in range(start_year, end_year+1):\n",
    "            indices += [jan1_idx + i for i in range(365)]\n",
    "            jan1_idx += 365\n",
    "            if calendar.isleap(year):\n",
    "                jan1_idx += 1\n",
    "        return arr[indices]\n",
    "    elif not southern_hem:\n",
    "        indices = []\n",
    "        jan1_idx = 0\n",
    "        for year in range(start_year, end_year+1):\n",
    "            indices += [jan1_idx + i for i in range(365)]\n",
    "            jan1_idx += 365\n",
    "            if calendar.isleap(year):\n",
    "                jan1_idx += 1\n",
    "        return arr[indices]\n",
    "    else:\n",
    "        indices = []\n",
    "        jul1_idx = 0\n",
    "        for year in range(start_year-1, end_year):\n",
    "            indices += [jul1_idx + i for i in range(365)]\n",
    "            jul1_idx += 365\n",
    "            if calendar.isleap(year):\n",
    "                jul1_idx += 1\n",
    "        return arr[indices]\n",
    "\n",
    "def get_eravar(varname, latlon, start_year, end_year, southern_hem=False, extralong=False):\n",
    "    model = 'ERA5'\n",
    "    dataset = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\n",
    "    gee_geom = ee.Geometry.Point((latlon[1], latlon[0]))\n",
    "    if extralong:\n",
    "        data_vars = dataset.select(varname).filter(ee.Filter.date('{0}-07-01'.format(start_year-1), '{0}-01-01'.format(end_year+1)))\n",
    "    elif not southern_hem:\n",
    "        data_vars = dataset.select(varname).filter(ee.Filter.date('{0}-01-01'.format(start_year), '{0}-01-01'.format(end_year+1)))\n",
    "    else:\n",
    "        data_vars = dataset.select(varname).filter(ee.Filter.date('{0}-07-01'.format(start_year-1), '{0}-07-01'.format(end_year)))\n",
    "    result = [i[4] for i in data_vars.getRegion(gee_geom, 2500, 'epsg:4326').getInfo()[1:]]\n",
    "    return removeLeapDays(np.array(result), start_year, end_year, extralong=extralong, southern_hem=southern_hem)\n",
    "\n",
    "def get_var(varname, model, loc_id, start_year, end_year, southern_hem=False, extralong=False, scenario='ssp585'):\n",
    "    scenario = [scenario, 'historical'][int(start_year < 2015)]\n",
    "    dataset = datasets[model]\n",
    "    if extralong:\n",
    "        dates = ('{0}-07-01'.format(start_year-1), '{0}-12-31'.format(end_year))\n",
    "    elif not southern_hem:\n",
    "        dates = ('{0}-01-01'.format(start_year), '{0}-12-31'.format(end_year))\n",
    "    else:\n",
    "        dates = ('{0}-07-01'.format(start_year-1), '{0}-06-30'.format(end_year))\n",
    "    \n",
    "    ds = dataset.get_timeseries(dates, loc_id)\n",
    "    if YEARLENGTH[model] == 366:\n",
    "        return removeLeapDays(np.array(ds), start_year, end_year, extralong=extralong, southern_hem=southern_hem)\n",
    "    else:\n",
    "        return np.array(ds)\n",
    "\n",
    "def quarters(d, start_year, end_year, southern_hem=False):\n",
    "    q2 = []  # 60-151\n",
    "    q3 = []  # 152-243\n",
    "    q4 = []  # 244-334\n",
    "    q1 = []  # 335-59\n",
    "    if not southern_hem:\n",
    "        jan1_idx = 365\n",
    "        for year in range(start_year, end_year):\n",
    "            tmp = np.concatenate((d[jan1_idx - 365 : jan1_idx - 365 + 60], d[jan1_idx + 335 : jan1_idx + 365]), axis=0)\n",
    "            q1.append(tmp)\n",
    "            q2.append(d[jan1_idx + 60 : jan1_idx + 152])\n",
    "            q3.append(d[jan1_idx + 152 : jan1_idx + 244])\n",
    "            q4.append(d[jan1_idx + 244 : jan1_idx + 335])\n",
    "\n",
    "            jan1_idx += 365 + [0, 0][int(False and calendar.isleap(year))]\n",
    "        mam_res = np.vstack(q2)\n",
    "        jja_res = np.vstack(q3)\n",
    "        son_res = np.vstack(q4)\n",
    "        djf_res = np.vstack(q1)\n",
    "    else:\n",
    "        jul1_idx = 365\n",
    "        for year in range(start_year, end_year+1):\n",
    "            tmp = np.concatenate((d[jul1_idx - 365 : jul1_idx - 365 + 60], d[jul1_idx + 335 : jul1_idx + 365]), axis=0)\n",
    "            q3.append(tmp)\n",
    "            q4.append(d[jul1_idx + 60 : jul1_idx + 152])\n",
    "            q1.append(d[jul1_idx + 152 : jul1_idx + 244])\n",
    "            q2.append(d[jul1_idx + 244 : jul1_idx + 335])\n",
    "\n",
    "            jul1_idx += 365 + [0, 0][int(False and calendar.isleap(year))]\n",
    "        mam_res = np.vstack(q4)\n",
    "        jja_res = np.vstack(q1)\n",
    "        son_res = np.vstack(q2)\n",
    "        djf_res = np.vstack(q3)\n",
    "    return mam_res, jja_res, son_res, djf_res\n",
    "    \n",
    "def seasonal_means(d):\n",
    "    q = quarters(d, HIST_START, HIST_END)\n",
    "    return np.array([np.mean(q[0], axis=1), np.mean(q[1], axis=1), np.mean(q[2], axis=1), np.mean(q[3], axis=1)])\n",
    "\n",
    "def calibration_function(hist_obs, hist_mod):\n",
    "# Calibration functions are P-P plots of historical and modeled values\n",
    "\n",
    "    source = np.sort(hist_obs.flatten())\n",
    "    target= np.sort(hist_mod.flatten())\n",
    "   \n",
    "    if (np.max(source) == 0 and np.min(source) == 0):\n",
    "        return np.arange(0, target.size) / target.size\n",
    "    if (np.max(target) == 0 and np.min(target) == 0):\n",
    "        return np.arange(0, source.size) / source.size\n",
    "    new_indices = []\n",
    "\n",
    "    for target_idx, target_value in enumerate(target):\n",
    "        if target_idx < len(source):\n",
    "            source_value = source[target_idx]\n",
    "            if source_value > target[-1]:\n",
    "                new_indices.append(target.size - 1)\n",
    "            else:\n",
    "                new_indices.append(np.argmax(target >= source_value))\n",
    "    return np.array(new_indices) / source.size\n",
    "\n",
    "def calibrate_component(uncalibrated_data, calibration_fxn):\n",
    "    N = len(uncalibrated_data)\n",
    "    unsorted_uncalib = [(i, idx) for idx, i in enumerate(uncalibrated_data)]\n",
    "    sorted_uncalib = sorted(unsorted_uncalib)\n",
    "    result = [0] * N\n",
    "    for j in range(N):\n",
    "        X_j = j / (N + 1)\n",
    "        Y_jprime = calibration_fxn[math.floor(X_j * len(calibration_fxn))]\n",
    "        jprime = math.floor(Y_jprime * (N + 1))\n",
    "        result[sorted_uncalib[j][1]] = sorted_uncalib[min(len(sorted_uncalib)-1, jprime)][0]\n",
    "    return result\n",
    "\n",
    "def calibrate(uncalibrated_data, calibration_fxn):\n",
    "    mam = []\n",
    "    jja = []\n",
    "    son = []\n",
    "    djf = []\n",
    "    mam_idx = []\n",
    "    jja_idx = []\n",
    "    son_idx = []\n",
    "    djf_idx = []\n",
    "    for idx, i in enumerate(uncalibrated_data):\n",
    "        if idx % 365 >= 60 and idx % 365 < 152:\n",
    "            mam.append(uncalibrated_data[idx])\n",
    "            mam_idx.append(idx)\n",
    "        elif idx % 365 >= 152 and idx % 365 < 244:\n",
    "            jja.append(uncalibrated_data[idx])\n",
    "            jja_idx.append(idx)\n",
    "        elif idx % 365 >= 244 and idx % 365 < 335:\n",
    "            son.append(uncalibrated_data[idx])\n",
    "            son_idx.append(idx)\n",
    "        else:\n",
    "            djf.append(uncalibrated_data[idx])\n",
    "            djf_idx.append(idx)\n",
    "    \n",
    "    mam_calib = calibrate_component(np.array(mam), calibration_fxn[0])\n",
    "    jja_calib = calibrate_component(np.array(jja), calibration_fxn[1])\n",
    "    son_calib = calibrate_component(np.array(son), calibration_fxn[2])\n",
    "    djf_calib = calibrate_component(np.array(djf), calibration_fxn[3])\n",
    "    \n",
    "    result = [0] * len(uncalibrated_data)\n",
    "    for i in range(len(mam_idx)):\n",
    "        result[mam_idx[i]] = mam_calib[i]\n",
    "    for i in range(len(jja_idx)):\n",
    "        result[jja_idx[i]] = jja_calib[i]\n",
    "    for i in range(len(son_idx)):\n",
    "        result[son_idx[i]] = son_calib[i]\n",
    "    for i in range(len(djf_idx)):\n",
    "        result[djf_idx[i]] = djf_calib[i]\n",
    "\n",
    "    return np.array(result)\n",
    "\n",
    "def get_gamma(count, size):\n",
    "    return np.random.gamma(shape = count + 0.5, size=size)\n",
    "def get_beta(count, num, size):\n",
    "    return np.random.beta(a = count + 0.5, b = num - count + 0.5, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b53602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_URI = {}\n",
    "with open('modelinfo.csv', 'r') as ifile:\n",
    "    for line in ifile.readlines():\n",
    "        items = [i.strip() for i in line.split(',')]\n",
    "        model, scenario, varname, the_uri = items\n",
    "        MODEL_URI[(model, scenario, varname)] = the_uri\n",
    "        \n",
    "def uri(model, scenario, varname):\n",
    "    return MODEL_URI[(model, scenario, varname)]\n",
    "\n",
    "def s3open(path):\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    return s3fs.S3Map(path, s3=fs)\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, varname, model, scenario, start_idx, end_idx):\n",
    "        self.varname =varname\n",
    "        self.model = model\n",
    "        self.scenario = scenario\n",
    "        \n",
    "        print('Extracting {0} {1} {2}'.format(varname, scenario, model))\n",
    "        \n",
    "        thefile = s3open(uri(model, scenario, varname))\n",
    "        ds = xr.open_mfdataset([thefile], engine='zarr', parallel=True)\n",
    "        ds = ds[varname].sel(time=slice(['{0}-07-01'.format(FUTURE_START), '{0}-01-01'.format(HIST_START)][int(scenario=='historical')], ['{0}-12-31'.format(FUTURE_END), '{0}-12-31'.format(HIST_END)][int(scenario=='historical')])).sel(lat=[i[0] for i in list(CITYLATLON.values())[start_idx:end_idx]], lon=[[i[1], i[1]+360][int(i[1]<0)] for i in list(CITYLATLON.values())[start_idx:end_idx]], method='nearest')\n",
    "\n",
    "        self.data = VARIABLES[varname]['nex_transform'](ds.to_numpy())\n",
    "\n",
    "        self.times = [str(d)[:10] for d in ds.time.data]\n",
    "\n",
    "        \n",
    "    def get_timeseries(self, dates, loc_id):\n",
    "        timestart_idx = self.times.index(dates[0])\n",
    "        timeend_idx = self.times.index(dates[1]) + 1\n",
    "        return self.data[timestart_idx:timeend_idx, loc_id, loc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce85c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histobs(varname, latlon):\n",
    "    def relhum(T, Tdp):\n",
    "        T = T.astype(np.float64)\n",
    "        Tdp = Tdp.astype(np.float64)\n",
    "        numerator = np.exp(17.625 * Tdp / (243.04 + Tdp))\n",
    "        denominator = np.exp(17.625 * T / (243.04 + T))\n",
    "        return 100 * numerator / denominator\n",
    "\n",
    "    #print('  Getting historical {0}'.format(varname))\n",
    "    if varname == 'hurs':\n",
    "        era_dewpoint = VARIABLES['tasmax']['era_transform'](get_eravar('dewpoint_2m_temperature', latlon, HIST_START, HIST_END, southern_hem=False))\n",
    "        era_maxtemp = VARIABLES['tasmax']['era_transform'](get_eravar(VARIABLES['tasmax']['era_varname'], latlon, HIST_START, HIST_END, southern_hem=False))\n",
    "        hist_obs = relhum(era_maxtemp, era_dewpoint)\n",
    "    elif varname == 'sfcWind':\n",
    "        era_windspeedu = VARIABLES['sfcWind']['era_transform'](get_eravar('u_component_of_wind_10m', latlon, HIST_START, HIST_END, southern_hem=False))\n",
    "        success = False\n",
    "        counter = 0\n",
    "        while not success:\n",
    "            try:\n",
    "                era_windspeedv = VARIABLES['sfcWind']['era_transform'](get_eravar('v_component_of_wind_10m', latlon, HIST_START, HIST_END, southern_hem=False))\n",
    "                success = True\n",
    "            except:\n",
    "                if counter == 10:\n",
    "                    return None\n",
    "                else:\n",
    "                    print('trying again', counter)\n",
    "                    counter += 1\n",
    "        hist_obs = np.sqrt(np.power(era_windspeedu, 2) + np.power(era_windspeedv, 2))\n",
    "    else:\n",
    "        hist_obs = VARIABLES[varname]['era_transform'](get_eravar(VARIABLES[varname]['era_varname'], latlon, HIST_START, HIST_END, southern_hem=False))\n",
    "    return hist_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ae03622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Location:\n",
    "    def __init__(self, params):\n",
    "        name, loc_id, latlon, hist_obs, hist_mods, varname = params\n",
    "        self.name = name\n",
    "        self.loc_id = loc_id\n",
    "        self.latlon = latlon\n",
    "        self.hist_observed = hist_obs\n",
    "        self.hist_modeled = hist_mods\n",
    "        self.best_models = None\n",
    "        self.calib_fxns = None\n",
    "\n",
    "\n",
    "        #hist_mods = {}\n",
    "        rmsds = []\n",
    "        for model in MODELS[varname]:\n",
    "            #print('    Getting {0}'.format(model))\n",
    "            hist_obs = self.hist_observed#VARIABLES[varname]['nex_transform'](get_var(varname, model, self.loc_id, HIST_START, HIST_END, southern_hem=False, scenario='historical'))\n",
    "            #hist_mods[model] = datasets[model].get_timeseries(('{0}-01-01'.format(HIST_START), '{0}-12-31'.format(HIST_END)), loc_id)#hist_mod\n",
    "            hist_mod = hist_mods[model]\n",
    "            rmsds.append((get_rmsd(hist_obs, hist_mod), model))\n",
    "        rmsds.sort()\n",
    "\n",
    "        best_models = []\n",
    "        families = []\n",
    "        idx = 0\n",
    "        while len(best_models) < NUM_BEST_MODELS:\n",
    "            if not MODEL_FAMILY[rmsds[idx][1]] in families:\n",
    "                best_models.append(rmsds[idx][1])\n",
    "                families.append(MODEL_FAMILY[rmsds[idx][1]])\n",
    "            idx += 1\n",
    "\n",
    "        #for m in best_models:\n",
    "        #    print(m, [i[0] for i in rmsds if i[1]==m][0])\n",
    "        #best_models = []\n",
    "        #for idx in range(min(NUM_BEST_MODELS, len(MODELS[varname]))):\n",
    "        #    best_models.append(rmsds[idx][1])\n",
    "\n",
    "        self.hist_modeled = hist_mods\n",
    "        self.best_models = best_models\n",
    "\n",
    "\n",
    "    # Get calibration functions\n",
    "        #print('  Getting calibration functions')\n",
    "        self.calib_fxns = {}\n",
    "        hist_obs = self.hist_observed\n",
    "        hist_mod = self.hist_modeled\n",
    "        for model in self.best_models:\n",
    "            o_quarters = quarters(hist_obs, HIST_START, HIST_END)\n",
    "            m_quarters = quarters(hist_mod[model], HIST_START, HIST_END)\n",
    "            self.calib_fxns[model] = [calibration_function(o_quarters[i].flatten(), m_quarters[i].flatten()) for i in range(4)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0cae7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_city(lat, lon, loc_id, hist_obs):\n",
    "    loc = client.submit(Location, cityname, loc_id, (lat, lon), hist_obs[loc_id], {m: datasets[m].get_timeseries(('{0}-01-01'.format(HIST_START), '{0}-12-31'.format(HIST_END)), loc_id) for m in MODELS[varname]})\n",
    "    return((loc.best_models, loc.calib_fxns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "779f1561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histobs(varname, latlon):\n",
    "    def relhum(T, Tdp):\n",
    "        T = T.astype(np.float64)\n",
    "        Tdp = Tdp.astype(np.float64)\n",
    "        numerator = np.exp(17.625 * Tdp / (243.04 + Tdp))\n",
    "        denominator = np.exp(17.625 * T / (243.04 + T))\n",
    "        return 100 * numerator / denominator\n",
    "\n",
    "    #print('  Getting historical {0}'.format(varname))\n",
    "    if varname == 'hurs':\n",
    "        era_dewpoint = VARIABLES['tasmax']['era_transform'](get_eravar('dewpoint_2m_temperature', latlon, HIST_START, HIST_END, southern_hem=False))\n",
    "        era_maxtemp = VARIABLES['tasmax']['era_transform'](get_eravar(VARIABLES['tasmax']['era_varname'], latlon, HIST_START, HIST_END, southern_hem=False))\n",
    "        hist_obs = relhum(era_maxtemp, era_dewpoint)\n",
    "    elif varname == 'sfcWind':\n",
    "        era_windspeedu = VARIABLES['sfcWind']['era_transform'](get_eravar('u_component_of_wind_10m', latlon, HIST_START, HIST_END, southern_hem=False))\n",
    "        success = False\n",
    "        counter = 0\n",
    "        while not success:\n",
    "            try:\n",
    "                era_windspeedv = VARIABLES['sfcWind']['era_transform'](get_eravar('v_component_of_wind_10m', latlon, HIST_START, HIST_END, southern_hem=False))\n",
    "                success = True\n",
    "            except:\n",
    "                if counter == 10:\n",
    "                    return None\n",
    "                else:\n",
    "                    print('trying again', counter)\n",
    "                    counter += 1\n",
    "        hist_obs = np.sqrt(np.power(era_windspeedu, 2) + np.power(era_windspeedv, 2))\n",
    "    else:\n",
    "        hist_obs = VARIABLES[varname]['era_transform'](get_eravar(VARIABLES[varname]['era_varname'], latlon, HIST_START, HIST_END, southern_hem=False))\n",
    "    return hist_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c4cc5c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1f2e18bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8141bf92367464dbd040df65db1a087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭────────────────────────────────────────── <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Package Info</span> ──────────────────────────────────────────╮\n",
       "│                             ╷                                                                    │\n",
       "│  <span style=\"font-weight: bold\"> Package                   </span>│<span style=\"font-weight: bold\"> Note                                                             </span>  │\n",
       "│ ╶───────────────────────────┼──────────────────────────────────────────────────────────────────╴ │\n",
       "│   coiled_local_halfdeg      │ Source wheel built from C:\\Users\\theodore.wong\\halfdeg             │\n",
       "│                             ╵                                                                    │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭────────────────────────────────────────── \u001b[1;32mPackage Info\u001b[0m ──────────────────────────────────────────╮\n",
       "│                             ╷                                                                    │\n",
       "│  \u001b[1m \u001b[0m\u001b[1mPackage                  \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mNote                                                            \u001b[0m\u001b[1m \u001b[0m  │\n",
       "│ ╶───────────────────────────┼──────────────────────────────────────────────────────────────────╴ │\n",
       "│   coiled_local_halfdeg      │ Source wheel built from C:\\Users\\theodore.wong\\halfdeg             │\n",
       "│                             ╵                                                                    │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d9de126aff4649b0f39224f60f5e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'MRI-ESM2-0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:20\u001b[0m\n",
      "File \u001b[1;32m~\\halfdeg\\halfdegenv\\lib\\site-packages\\distributed\\client.py:2393\u001b[0m, in \u001b[0;36mClient.gather\u001b[1;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[0;32m   2390\u001b[0m     local_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2392\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m-> 2393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2394\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gather\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2396\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2399\u001b[0m \u001b[43m        \u001b[49m\u001b[43masynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2400\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[50], line 19\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     hist_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhist_observed\u001b[38;5;66;03m#VARIABLES[varname]['nex_transform'](get_var(varname, model, self.loc_id, HIST_START, HIST_END, southern_hem=False, scenario='historical'))\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m#hist_mods[model] = datasets[model].get_timeseries(('{0}-01-01'.format(HIST_START), '{0}-12-31'.format(HIST_END)), loc_id)#hist_mod\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     hist_mod \u001b[38;5;241m=\u001b[39m hist_mods[model]\n\u001b[0;32m     20\u001b[0m     rmsds\u001b[38;5;241m.\u001b[39mappend((get_rmsd(hist_obs, hist_mod), model))\n\u001b[0;32m     21\u001b[0m rmsds\u001b[38;5;241m.\u001b[39msort()\n",
      "\u001b[1;31mKeyError\u001b[0m: 'MRI-ESM2-0'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cluster = coiled.Cluster(n_workers=25)\n",
    "client = cluster.get_client()\n",
    "for varname in ['pr', 'sfcWind']:#VARIABLES:\n",
    "    \n",
    "    for idx_start, idx_end in [(i * 12, (i+1)*12) for i in range(83)]:\n",
    "        \n",
    "        datasets = client.gather({\n",
    "            (varname, model): client.submit(Dataset, varname, model, 'historical', idx_start, idx_end) for model in MODELS[varname]\n",
    "        })\n",
    "        \n",
    "        city_params = []\n",
    "        for cityname in list(CITYLATLON.keys())[idx_start: idx_end]:\n",
    "            lat, lon, loc_id = CITYLATLON[cityname]\n",
    "            hist_obs = get_histobs(varname, (lat, lon))\n",
    "            hist_mod = {m: datasets[(varname, m)].get_timeseries(('{0}-01-01'.format(HIST_START), '{0}-12-31'.format(HIST_END)), loc_id-idx_start) for m in MODELS[varname]}\n",
    "            city_params.append((cityname, loc_id-idx_start, (lat, lon), hist_obs, hist_mod, varname))\n",
    "            \n",
    "        locs_futs = client.map(Location, city_params)\n",
    "\n",
    "        loclist = client.gather(locs_futs)\n",
    "        #This is list of result locs -- each has best_odels and calib_fxns\n",
    "        # need to use list index to get cityinfo which is in start-stop idxs\n",
    "        for idx, loc in enumerate(loclist):\n",
    "            best_models = loc.best_models\n",
    "            calib_fxns = loc.calib_fxns\n",
    "            with open('bestmodels_{0}.txt'.format(varname), 'a') as ofile:\n",
    "                for m in best_models:\n",
    "                    ofile.write('{0}\\t{1}\\t{2}\\t{3}\\n'.format(idx+idx_start, varname, m, json.dumps([a.tolist() for a in calib_fxns[m]])))\n",
    "            #cluster.shutdown()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "67d40a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "30d6862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95bd309c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pr'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5bbed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ef10b9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GFDL-ESM4': <__main__.Dataset at 0x1ce60cd42b0>,\n",
       " 'CanESM5': <__main__.Dataset at 0x1ce3aaeb9a0>,\n",
       " 'MRI-ESM2-0': <Future: cancelled, type: __main__.Dataset, key: Dataset-ce4d43fa167265c7915596c8c10eaf43>,\n",
       " 'IPSL-CM6A-LR': <Future: cancelled, type: __main__.Dataset, key: Dataset-706dbe19d75f2112886f24d16feedee9>,\n",
       " 'EC-Earth3-Veg-LR': <Future: cancelled, type: __main__.Dataset, key: Dataset-cec8588e1415bcf10adabe65bc26a1b1>}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce2997f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to retrieve results use this:\n",
    "calib_fxns = {}\n",
    "with open('bestmodels.txt', 'r') as ifile:\n",
    "    for line in ifile.readlines():\n",
    "        items = [i.strip() for i in line.split('\\t')]\n",
    "        calib_fxns[(items[0], items[3], items[4])] = {items[5]: items[6], items[7]: items[8], items[9]: items[10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab1db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b918967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7dd43ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFDL-CM4: min modeled value does not exceed observed 10th percentile  True\n",
      "GFDL-CM4: max modeled value does not exceed observed 90th percentile  True\n",
      "CanESM5: min modeled value does not exceed observed 10th percentile  True\n",
      "CanESM5: max modeled value does not exceed observed 90th percentile  True\n",
      "ACCESS-CM2: min modeled value does not exceed observed 10th percentile  True\n",
      "ACCESS-CM2: max modeled value does not exceed observed 90th percentile  True\n",
      "GFDL-CM4: min modeled value does not exceed observed 10th percentile  True\n",
      "GFDL-CM4: max modeled value does not exceed observed 90th percentile  True\n",
      "CanESM5: min modeled value does not exceed observed 10th percentile  True\n",
      "CanESM5: max modeled value does not exceed observed 90th percentile  True\n",
      "ACCESS-CM2: min modeled value does not exceed observed 10th percentile  True\n",
      "ACCESS-CM2: max modeled value does not exceed observed 90th percentile  True\n",
      "GFDL-CM4: min modeled value does not exceed observed 10th percentile  True\n",
      "GFDL-CM4: max modeled value does not exceed observed 90th percentile  True\n",
      "CanESM5: min modeled value does not exceed observed 10th percentile  True\n",
      "CanESM5: max modeled value does not exceed observed 90th percentile  True\n",
      "ACCESS-CM2: min modeled value does not exceed observed 10th percentile  True\n",
      "ACCESS-CM2: max modeled value does not exceed observed 90th percentile  True\n",
      "GFDL-CM4: min modeled value does not exceed observed 10th percentile  True\n",
      "GFDL-CM4: max modeled value does not exceed observed 90th percentile  True\n",
      "CanESM5: min modeled value does not exceed observed 10th percentile  False\n",
      "CanESM5: max modeled value does not exceed observed 90th percentile  True\n",
      "ACCESS-CM2: min modeled value does not exceed observed 10th percentile  False\n",
      "ACCESS-CM2: max modeled value does not exceed observed 90th percentile  True\n"
     ]
    }
   ],
   "source": [
    "for quarter in range(4):\n",
    "    obs_10 = np.percentile(quarters(hist_obs_tx, HIST_START, HIST_END)[quarter], 10)\n",
    "    obs_90 = np.percentile(quarters(hist_obs_tx, HIST_START, HIST_END)[quarter], 90)\n",
    "    for model in best_models_tx:\n",
    "        mod = quarters(hist_mods_tx[model] - 273.15, HIST_START, HIST_END)[quarter].flatten()\n",
    "        print('{0}: min modeled value does not exceed observed 10th percentile  {1}'.format(model, min(mod) <= obs_10))\n",
    "        print('{0}: max modeled value does not exceed observed 90th percentile  {1}'.format(model, max(mod) >= obs_90))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2847140a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (halfdegenv)",
   "language": "python",
   "name": "halfdegenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
