{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abdba210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "%matplotlib inline\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import datetime, calendar\n",
    "\n",
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "\n",
    "#ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "254785d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INFO = {'UKESM1-0-LL': 'HadAM',\n",
    " 'NorESM2-MM': 'CCM',\n",
    " 'NorESM2-LM': 'CCM',\n",
    " 'MRI-ESM2-0': 'UCLA GCM',\n",
    " 'MPI-ESM1-2-LR': 'ECMWF',\n",
    " 'MPI-ESM1-2-HR': 'ECMWF',\n",
    " 'MIROC6': 'MIROC',\n",
    " 'MIROC-ES2L': 'MIROC',\n",
    " 'KIOST-ESM': 'GFDL',\n",
    " 'KACE-1-0-G': 'HadAM',\n",
    " 'IPSL-CM6A-LR': 'IPSL',\n",
    " 'INM-CM5-0': 'INM',\n",
    " 'INM-CM4-8': 'INM',\n",
    " 'HadGEM3-GC31-MM': 'HadAM',\n",
    " 'HadGEM3-GC31-LL': 'HadAM',\n",
    " 'GFDL-ESM4': 'GFDL',\n",
    " 'GFDL-CM4_gr2': 'GFDL',\n",
    " 'GFDL-CM4': 'GFDL',\n",
    " 'FGOALS-g3': 'CCM',\n",
    " 'EC-Earth3-Veg-LR': 'ECMWF',\n",
    " 'EC-Earth3': 'ECMWF',\n",
    " 'CanESM5': 'CanAM',\n",
    " 'CNRM-ESM2-1': 'ECMWF',\n",
    " 'CNRM-CM6-1': 'ECMWF',\n",
    " 'CMCC-ESM2': 'CCM',\n",
    " 'CMCC-CM2-SR5': 'CCM',\n",
    " 'BCC-CSM2-MR': 'CCM',\n",
    " 'ACCESS-ESM1-5': 'HadAM',\n",
    " 'ACCESS-CM2': 'HadAM',\n",
    " 'TaiESM1': 'CCM',\n",
    "}\n",
    "\n",
    "EXCLUDED_MODELS = ['GFDL-CM4_gr2','ERA5']\n",
    "\n",
    "MODELS = [i for i in MODEL_INFO.keys() if not i in EXCLUDED_MODELS]\n",
    "\n",
    "\n",
    "CAMPINAS_LATLON = (-22.907104, -47.063240)  # Campinas\n",
    "\n",
    "HIST_START = 1980\n",
    "HIST_END = 2014\n",
    "FUTURE_START = 2050\n",
    "FUTURE_END = 2070\n",
    "\n",
    "PERCENTILE_STARTYEAR = 1980\n",
    "PERCENTILE_ENDYEAR = 2019\n",
    "\n",
    "STUDY_THRESH = 40\n",
    "\n",
    "NUM_BEST_MODELS = 3\n",
    "\n",
    "shift_years = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "66e61532",
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = {\n",
    "    'tasmax': {\n",
    "        'era_varname': 'maximum_2m_air_temperature',\n",
    "        'nex_transform': lambda x: x - 273.5,\n",
    "        'era_transform': lambda x: x - 273.5\n",
    "    },\n",
    "    'tasmin': {\n",
    "        'era_varname': 'minimum_2m_air_temperature',\n",
    "        'nex_transform': lambda x: x - 273.5,\n",
    "        'era_transform': lambda x: x - 273.5\n",
    "    },\n",
    "    'pr': {\n",
    "        'era_varname': 'total_precipitation',\n",
    "        'nex_transform': lambda x: x * 86400,\n",
    "        'era_transform': lambda x: x * 1000\n",
    "    }   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "785e9087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calendardate_percentiles(nex_varname, q, latlon, sh_hem=False):\n",
    "    era_varname = VARIABLES[nex_varname]['era_varname']\n",
    "    hist_start = PERCENTILE_STARTYEAR\n",
    "    hist_end = PERCENTILE_ENDYEAR\n",
    "    allyears = []\n",
    "    for year in range(hist_start, hist_end):\n",
    "        allyears.append(VARIABLES[nex_varname]['era_transform'](get_var(era_varname, 'ERA5', latlon, start_year=year, end_year=year, southern_hem=False)))\n",
    "    if not sh_hem:\n",
    "        return np.percentile(np.vstack(allyears), q, axis=0)\n",
    "    else:\n",
    "        res = np.percentile(np.vstack(allyears), q, axis=0)\n",
    "        return np.concatenate([res[152:], res[:152]])\n",
    "\n",
    "def wholeyear_percentile(nex_varname, q, latlon):\n",
    "    era_varname = VARIABLES[nex_varname]['era_varname']\n",
    "    hist_start = PERCENTILE_STARTYEAR\n",
    "    hist_end = PERCENTILE_ENDYEAR\n",
    "    allyears = []\n",
    "    for year in range(hist_start, hist_end):\n",
    "        allyears.append(VARIABLES[nex_varname]['era_transform'](get_var(era_varname, 'ERA5', latlon, start_year=year, end_year=year, southern_hem=False)))\n",
    "    return np.percentile(np.concatenate(allyears).flatten(), q)\n",
    "\n",
    "def yearextreme_percentile(nex_varname, q, latlon, wantmax):\n",
    "    era_varname = VARIABLES[nex_varname]['era_varname']\n",
    "    hist_start = PERCENTILE_STARTYEAR\n",
    "    hist_end = PERCENTILE_ENDYEAR\n",
    "    allyears = []\n",
    "    for year in range(hist_start, hist_end):\n",
    "        allyears.append([np.min, np.max][int(wantmax)](VARIABLES[nex_varname]['era_transform'](get_var(era_varname, 'ERA5', latlon, start_year=year, end_year=year, southern_hem=False))))\n",
    "    return np.percentile(np.array(allyears), q)\n",
    "\n",
    "def d2j(datestring):\n",
    "    d = datetime.date.fromisoformat(datestring)\n",
    "    jday = d.timetuple().tm_yday\n",
    "    if calendar.isleap(d.year) and jday > 59:\n",
    "        jday -= 1\n",
    "    return jday\n",
    "\n",
    "def get_rmsd(d1, d2):\n",
    "    c1 = seasonal_means(d1)\n",
    "    c2 = seasonal_means(d2)\n",
    "    return np.sqrt(np.mean(np.sum((c1 - c2)**2)))\n",
    "\n",
    "def count_runs(tf_array, min_runsize):\n",
    "    falses = np.zeros(tf_array.shape[0]).reshape((tf_array.shape[0],1))\n",
    "    extended_a = np.concatenate([[0], tf_array, [0]])\n",
    "    df = np.diff(extended_a)\n",
    "    starts = np.nonzero(df == 1)[0]\n",
    "    ends = np.nonzero(df == -1)[0]\n",
    "    count = 0\n",
    "    for idx in range(starts.size):\n",
    "        if ends[idx] - starts[idx] >= min_runsize:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def get_var(varname, model, latlon, start_year, end_year, southern_hem=False, scenario='ssp585'):\n",
    "    def removeLeapDays(arr, southern_hem=False):\n",
    "        if not southern_hem:\n",
    "            indices = []\n",
    "            jan1_idx = 0\n",
    "            for year in range(start_year, end_year+1):\n",
    "                indices += [jan1_idx + i for i in range(365)]\n",
    "                jan1_idx += 365\n",
    "                if calendar.isleap(year):\n",
    "                    jan1_idx += 1\n",
    "            return arr[indices]\n",
    "        else:\n",
    "            indices = []\n",
    "            jul1_idx = 0\n",
    "            for year in range(start_year-1, end_year):\n",
    "                indices += [jul1_idx + i for i in range(183)]\n",
    "                jul1_idx += 183\n",
    "                if calendar.isleap(year):\n",
    "                    jul1_idx += 1\n",
    "                indices += [jul1_idx + i for i in range(182)]\n",
    "                jul1_idx += 182\n",
    "            return arr[indices]\n",
    "    if model != 'ERA5' and start_year < 2015 and end_year >= 2015:\n",
    "        raise Exception(\"Requesting hist and non-hist variables in one query\")\n",
    "    if model == 'ERA5':\n",
    "        dataset = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\n",
    "    else:\n",
    "        dataset = ee.ImageCollection('NASA/GDDP-CMIP6').filter(ee.Filter.eq('model', model)).filter(ee.Filter.eq('scenario', [scenario, 'historical'][int(end_year<2015)]))\n",
    "    gee_geom = ee.Geometry.Point((latlon[1], latlon[0]))\n",
    "    if not southern_hem:\n",
    "        data_vars = dataset.select(varname).filter(ee.Filter.date('{0}-01-01'.format(start_year), '{0}-01-01'.format(end_year+ 1)))\n",
    "        result = [i[4] for i in data_vars.getRegion(gee_geom, 2500, 'epsg:4326').getInfo()[1:]]\n",
    "        return removeLeapDays(np.array(result), False)\n",
    "    else:\n",
    "        data_vars = dataset.select(varname).filter(ee.Filter.date('{0}-07-01'.format(start_year-1), '{0}-07-01'.format(end_year)))\n",
    "        result = [i[4] for i in data_vars.getRegion(gee_geom, 2500, 'epsg:4326').getInfo()[1:]]\n",
    "        return removeLeapDays(np.array(result), True)\n",
    "    \n",
    "def quarters(d, start_year, end_year, southern_hem=False):\n",
    "    q2 = []  # 60-151\n",
    "    q3 = []  # 152-243\n",
    "    q4 = []  # 244-334\n",
    "    q1 = []  # 335-59\n",
    "    if not southern_hem:\n",
    "        jan1_idx = 365\n",
    "        for year in range(start_year, end_year):\n",
    "            tmp = np.concatenate((d[jan1_idx - 365 : jan1_idx - 365 + 60], d[jan1_idx + 335 : jan1_idx + 365]), axis=0)\n",
    "            q1.append(tmp)\n",
    "            q2.append(d[jan1_idx + 60 : jan1_idx + 152])\n",
    "            q3.append(d[jan1_idx + 152 : jan1_idx + 244])\n",
    "            q4.append(d[jan1_idx + 244 : jan1_idx + 335])\n",
    "\n",
    "            jan1_idx += 365 + [0, 0][int(False and calendar.isleap(year))]\n",
    "        mam_res = np.vstack(q2)\n",
    "        jja_res = np.vstack(q3)\n",
    "        son_res = np.vstack(q4)\n",
    "        djf_res = np.vstack(q1)\n",
    "    else:\n",
    "        jul1_idx = 365\n",
    "        for year in range(start_year, end_year):\n",
    "            tmp = np.concatenate((d[jul1_idx - 365 : jul1_idx - 365 + 60], d[jul1_idx + 335 : jul1_idx + 365]), axis=0)\n",
    "            q3.append(tmp)\n",
    "            q4.append(d[jul1_idx + 60 : jul1_idx + 152])\n",
    "            q1.append(d[jul1_idx + 152 : jul1_idx + 244])\n",
    "            q2.append(d[jul1_idx + 244 : jul1_idx + 335])\n",
    "\n",
    "            jul1_idx += 365 + [0, 0][int(False and calendar.isleap(year))]\n",
    "        mam_res = np.vstack(q4)\n",
    "        jja_res = np.vstack(q1)\n",
    "        son_res = np.vstack(q2)\n",
    "        djf_res = np.vstack(q3)\n",
    "    return mam_res, jja_res, son_res, djf_res\n",
    "    \n",
    "def seasonal_means(d):\n",
    "    q = quarters(d, HIST_START, HIST_END)\n",
    "    return np.array([np.mean(q[0], axis=1), np.mean(q[1], axis=1), np.mean(q[2], axis=1), np.mean(q[3], axis=1)])\n",
    "\n",
    "def calibration_function(hist_obs, hist_mod):\n",
    "# Calibration functions are P-P plots of historical and modeled values\n",
    "\n",
    "    source = np.sort(hist_obs.flatten())\n",
    "    target= np.sort(hist_mod.flatten())\n",
    "   \n",
    "    if (np.max(source) == 0 and np.min(source) == 0):\n",
    "        return np.arange(0, target.size) / target.size\n",
    "    if (np.max(target) == 0 and np.min(target) == 0):\n",
    "        return np.arange(0, source.size) / source.size\n",
    "    new_indices = []\n",
    "\n",
    "    for target_idx, target_value in enumerate(target):\n",
    "        if target_idx < len(source):\n",
    "            source_value = source[target_idx]\n",
    "            if source_value > target[-1]:\n",
    "                new_indices.append(target.size - 1)\n",
    "            else:\n",
    "                new_indices.append(np.argmax(target >= source_value))\n",
    "    return np.array(new_indices) / source.size\n",
    "\n",
    "def calibrate_component(uncalibrated_data, calibration_fxn):\n",
    "    N = len(uncalibrated_data)\n",
    "    unsorted_uncalib = [(i, idx) for idx, i in enumerate(uncalibrated_data)]\n",
    "    sorted_uncalib = sorted(unsorted_uncalib)\n",
    "    result = [0] * N\n",
    "    for j in range(N):\n",
    "        X_j = j / (N + 1)\n",
    "        Y_jprime = calibration_fxn[math.floor(X_j * len(calibration_fxn))]\n",
    "        jprime = math.floor(Y_jprime * (N + 1))\n",
    "        result[sorted_uncalib[j][1]] = sorted_uncalib[min(len(sorted_uncalib)-1, jprime)][0]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def calibrate(uncalibrated_data, calibration_fxn):\n",
    "    mam = []\n",
    "    jja = []\n",
    "    son = []\n",
    "    djf = []\n",
    "    mam_idx = []\n",
    "    jja_idx = []\n",
    "    son_idx = []\n",
    "    djf_idx = []\n",
    "    for idx, i in enumerate(uncalibrated_data):\n",
    "        if idx % 365 >= 60 and idx % 365 < 152:\n",
    "            mam.append(uncalibrated_data[idx])\n",
    "            mam_idx.append(idx)\n",
    "        elif idx % 365 >= 152 and idx % 365 < 244:\n",
    "            jja.append(uncalibrated_data[idx])\n",
    "            jja_idx.append(idx)\n",
    "        elif idx % 365 >= 244 and idx % 365 < 335:\n",
    "            son.append(uncalibrated_data[idx])\n",
    "            son_idx.append(idx)\n",
    "        else:\n",
    "            djf.append(uncalibrated_data[idx])\n",
    "            djf_idx.append(idx)\n",
    "    \n",
    "    mam_calib = calibrate_component(np.array(mam), calibration_fxn[0])\n",
    "    jja_calib = calibrate_component(np.array(jja), calibration_fxn[1])\n",
    "    son_calib = calibrate_component(np.array(son), calibration_fxn[2])\n",
    "    djf_calib = calibrate_component(np.array(djf), calibration_fxn[3])\n",
    "    \n",
    "    result = [0] * len(uncalibrated_data)\n",
    "    for i in range(len(mam_idx)):\n",
    "        result[mam_idx[i]] = mam_calib[i]\n",
    "    for i in range(len(jja_idx)):\n",
    "        result[jja_idx[i]] = jja_calib[i]\n",
    "    for i in range(len(son_idx)):\n",
    "        result[son_idx[i]] = son_calib[i]\n",
    "    for i in range(len(djf_idx)):\n",
    "        result[djf_idx[i]] = djf_calib[i]\n",
    "\n",
    "    return np.array(result)\n",
    "\n",
    "def get_gamma(count, size):\n",
    "    return np.random.gamma(shape = count + 0.5, size=size)\n",
    "def get_beta(count, num, size):\n",
    "    return np.random.beta(a = count + 0.5, b = num - count + 0.5, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "a87ff323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hazard:\n",
    "    pass\n",
    "\n",
    "class Tempwave_simple(Hazard):\n",
    "    def __init__(self, varname, min_duration, threshold, want_gte=True):\n",
    "        if type(threshold) == np.ndarray and threshold.size % 365 != 0:\n",
    "            raise Exception('Comparison array length is not an integer multiple of 365')\n",
    "        self.varname = varname\n",
    "        self.want_gte = want_gte\n",
    "        self.min_duration = min_duration\n",
    "        self.threshold = threshold  # May be scalar or 365-long array\n",
    "        self.probmodel = 'Poisson'\n",
    "    def count(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if type(self.threshold) in (float, int, np.float64, np.int32):\n",
    "            threshold = self.threshold\n",
    "        else:   # type is np array\n",
    "            threshold = np.array([])\n",
    "            while threshold.size < data.size:\n",
    "                threshold = np.concatenate([threshold, self.threshold])\n",
    "        if self.want_gte:\n",
    "            tf_array = data >= threshold\n",
    "        else:\n",
    "            tf_array = data <= threshold\n",
    "        return count_runs(tf_array, self.min_duration)\n",
    "    \n",
    "class Heatwave_highlow(Hazard):\n",
    "    def __init__(self, hightemp, lowtemp, min_duration):\n",
    "        self.varname = 'tasmax+tasmin'\n",
    "        self.min_duration = min_duration\n",
    "        self.hightemp = hightemp\n",
    "        self.lowtemp = lowtemp\n",
    "        self.probmodel = 'Poisson'\n",
    "    def count(self, datalist):\n",
    "        data_tx = datalist[0]\n",
    "        data_tn = datalist[1]\n",
    "        if type(self.hightemp) in (float, int, np.float64, np.int32):\n",
    "            high_threshold = self.hightemp\n",
    "        else:   # type is np array\n",
    "            high_threshold = np.array([])\n",
    "            while high_threshold.size < data_tx.size:\n",
    "                high_threshold = np.concatenate([high_threshold, self.hightemp])\n",
    "        if type(self.lowtemp) in (float, int, np.float64, np.int32):\n",
    "            low_threshold = self.lowtemp\n",
    "        else:   # type is np array\n",
    "            low_threshold = np.array([])\n",
    "            while low_threshold.size < data_tn.size:\n",
    "                low_threshold = np.concatenate([low_threshold, self.lowtemp])\n",
    "        tf_array_tx = data_tx >= high_threshold\n",
    "        tf_array_tn = data_tn >= low_threshold\n",
    "        return count_runs(tf_array_tx * tf_array_tn, self.min_duration)\n",
    "\n",
    "class Hotdays_simple(Hazard):\n",
    "    def __init__(self, varname, min_temp):\n",
    "        self.varname = varname\n",
    "        self.min_temp = min_temp\n",
    "        self.probmodel = 'binomial'\n",
    "    def count(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')\n",
    "        byyear = data.reshape(data.size//365, 365)\n",
    "        return np.sum((np.max(byyear, axis=1) >= self.min_temp) * 1)\n",
    "    \n",
    "class Colddays_simple(Hazard):\n",
    "    def __init__(self, varname, max_temp):\n",
    "        self.varname = varname\n",
    "        self.max_temp = max_temp\n",
    "        self.probmodel = 'binomial'\n",
    "    def count(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')\n",
    "        byyear = data.reshape(data.size//365, 365)\n",
    "        return np.sum((np.min(byyear, axis=1) <= self.max_temp) * 1)\n",
    "    \n",
    "class Hotdays_inrange(Hazard):\n",
    "    def __init__(self, hightemp, lowtemp):\n",
    "        self.varname = 'tasmax'\n",
    "        self.hightemp = hightemp\n",
    "        self.lowtemp = lowtemp\n",
    "        self.probmodel = 'binomial'\n",
    "    def count(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')\n",
    "        tf_array_high = data <= self.hightemp\n",
    "        tf_array_low = data >= self.lowtemp\n",
    "        return runs(tf_array_high * tf_array_low, self.min_duration, 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "56cd6b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimate:\n",
    "    def __init__(self, location, hazard, future_start, future_end, sh_year):\n",
    "        self.location = location\n",
    "        self.hazard = hazard\n",
    "        self.future_start = future_start\n",
    "        self.future_end = future_end\n",
    "        self.sh_year = sh_year \n",
    "        self.estimate = {}\n",
    "        \n",
    "        for varname in self.hazard.varname.split('+'):\n",
    "            if not varname in list(self.location.hist_observed.keys()):\n",
    "                self.location.get_hist_simple(varname)\n",
    "        \n",
    "    def future_count(self):\n",
    "    # Gets future model outputs for three best models\n",
    "    # Calibrates based on stored model-specific quarterly calibration functions\n",
    "    # Calculates event count within future time series\n",
    "    # Draws 10,000 posterior rate parameters from appropriate Jeffrys prior distribution (parameterized with count)\n",
    "    # For each rate parameter, draw one event count\n",
    "    \n",
    "        fut_mod = {}\n",
    "        varnames = self.hazard.varname.split('+')\n",
    "        for varname in varnames:\n",
    "            fut_mod[varname] = {}\n",
    "            for model in self.location.best_models[varname]:\n",
    "                fut_mod[varname][model] = fut_mod[model] = VARIABLES[varname]['nex_transform'](get_var(varname, model, self.location.latlon, start_year=self.future_start, end_year=self.future_end + [0, -1][int(self.sh_year)], southern_hem=False))\n",
    "        best_models = []\n",
    "        for idx in range(NUM_BEST_MODELS):\n",
    "            best_models.append('+'.join([self.location.best_models[varname][idx] for varname in varnames]))\n",
    "        posterior_rateparams = {}\n",
    "        posterior_draws = {}\n",
    "        estimate = {}\n",
    "        for modelplus in best_models:\n",
    "            calib_data = []\n",
    "            for idx, varname in enumerate(varnames):\n",
    "                model = modelplus.split('+')[idx]\n",
    "                calib_data.append(np.array(calibrate(fut_mod[varname][model], self.location.calib_fxns[varname][model])))\n",
    "            if self.sh_year:\n",
    "                count = self.hazard.count(calib_data)\n",
    "            else:\n",
    "                count = self.hazard.count([cd[152:-213] for cd in calib_data])\n",
    "            if self.hazard.probmodel == 'Poisson':\n",
    "                posterior_rateparams[modelplus] = get_gamma(count, 10000)\n",
    "                posterior_draws[modelplus] = np.random.poisson(posterior_rateparams[modelplus], 10000)\n",
    "            else:  # self.hazard.probmodel == 'binomial'\n",
    "                posterior_rateparams[modelplus] = get_beta(count, self.future_end - self.future_start + 1, 10000)\n",
    "                posterior_draws[modelplus] = np.random.binomial(self.future_end - self.future_start + 1, posterior_rateparams[modelplus], 10000)\n",
    "            self.estimate[modelplus] = np.sum(posterior_draws[modelplus]) / 10000\n",
    "        if False:\n",
    "            fig, (ax0, ax1, ax2) = plt.subplots(1, 3, sharey=True, figsize=(18, 6))\n",
    "            hist = np.histogram(posterior_draws[list(posterior_draws.keys())[0]], bins=1000)\n",
    "            hist0 = ax0.bar(hist[1][:-1], hist[0]/np.sum(hist[0]))\n",
    "            ax0.set_title(list(posterior_draws.keys())[0])\n",
    "            ax0.set_xticks([i for i in range(22) if i % 5 == 0])\n",
    "            hist = np.histogram(posterior_draws[list(posterior_draws.keys())[1]], bins=1000)\n",
    "            hist1 = ax1.bar(hist[1][:-1], hist[0]/np.sum(hist[0]))\n",
    "            ax1.set_title(list(posterior_draws.keys())[1])\n",
    "            ax1.set_xticks([i for i in range(22) if i % 5 == 0])\n",
    "            hist = np.histogram(posterior_draws[list(posterior_draws.keys())[2]], bins=1000)\n",
    "            hist2 = ax2.bar(hist[1][:-1], hist[0]/np.sum(hist[0]))\n",
    "            ax2.set_title(list(posterior_draws.keys())[2])\n",
    "            ax2.set_xticks([i for i in range(22) if i % 5 == 0])\n",
    "            ax0.set_ylabel('Probability density')\n",
    "            ax0.set_xlabel('Future event count {0}-{1}'.format(self.future_start, self.future_end))\n",
    "            ax1.set_xlabel('Future event count {0}-{1}'.format(self.future_start, self.future_end))\n",
    "            ax2.set_xlabel('Future event count {0}-{1}'.format(self.future_start, self.future_end))\n",
    "            plt.show()\n",
    "        print()\n",
    "        for modelplus in best_models:\n",
    "            if self.hazard.probmodel == 'Poisson':\n",
    "                print('{0}: {1}'.format(modelplus, self.estimate[modelplus]))\n",
    "            else:\n",
    "                print('{0}: {1}'.format(modelplus, '{0:.1f}%'.format(self.estimate[modelplus] / (self.future_end - self.future_start + 1) * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "53a9c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Location:\n",
    "    def __init__(self, name, latlon):\n",
    "        self.name = name\n",
    "        self.latlon = latlon\n",
    "        self.hist_observed = {}\n",
    "        self.hist_modeled = {}\n",
    "        self.best_models = {}\n",
    "        self.calib_fxns = {}\n",
    "        \n",
    "    def get_hist_simple(self, varname):\n",
    "    # Gets historical observations from ERA5 Daily Aggregate (from GEE)\n",
    "    # Goes through all NEX-GDDP-CMIP6 models in GEE and gets historical model outputs\n",
    "    # Chooses three best models based on quarterly RMSD\n",
    "    \n",
    "        hist_obs = VARIABLES[varname]['era_transform'](get_var(VARIABLES[varname]['era_varname'], 'ERA5', self.latlon, HIST_START, HIST_END, southern_hem=False))\n",
    "        hist_mods = {}\n",
    "        rmsds = []\n",
    "        for model in MODELS:\n",
    "            hist_mod = VARIABLES[varname]['nex_transform'](get_var(varname, model, self.latlon, HIST_START, HIST_END, southern_hem=False))\n",
    "            hist_mods[model] = hist_mod\n",
    "            rmsds.append((get_rmsd(hist_obs, hist_mod), model))\n",
    "        rmsds.sort()\n",
    "        best_models = []\n",
    "        families = []\n",
    "        idx = 0\n",
    "        while len(best_models) < 3:\n",
    "            if not MODEL_INFO[rmsds[idx][1]] in families:\n",
    "                best_models.append(rmsds[idx][1])\n",
    "                families.append(MODEL_INFO[rmsds[idx][1]])\n",
    "            idx += 1\n",
    "\n",
    "        for m in best_models:\n",
    "            print(m, [i[0] for i in rmsds if i[1]==m][0])\n",
    "            \n",
    "        self.hist_observed[varname] = hist_obs\n",
    "        self.hist_modeled[varname] = hist_mods\n",
    "        self.best_models[varname] = best_models\n",
    "        \n",
    "    # Get calibration functions\n",
    "        self.calib_fxns[varname] = {}\n",
    "        hist_obs = self.hist_observed[varname]\n",
    "        hist_mod = self.hist_modeled[varname]\n",
    "        for model in self.best_models[varname]:\n",
    "            o_quarters = quarters(hist_obs, HIST_START, HIST_END)\n",
    "            m_quarters = quarters(hist_mod[model], HIST_START, HIST_END)\n",
    "            self.calib_fxns[varname][model] = [calibration_function(o_quarters[i].flatten(), m_quarters[i].flatten()) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "612d3d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int32"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.min(np.array([3,4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "77bc9750",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_90c = calendardate_percentiles('tasmax', 90, CAMPINAS_LATLON, sh_hem=True)\n",
    "low_90c = calendardate_percentiles('tasmin', 90, CAMPINAS_LATLON, sh_hem=True)\n",
    "cold_10y = wholeyear_percentile('tasmin', 10, CAMPINAS_LATLON)\n",
    "cold_10n = yearextreme_percentile('tasmin', 10, CAMPINAS_LATLON, False)\n",
    "campinas_hazards = [\n",
    "    {'name': 'Heat wave', 'obj': Heatwave_highlow(high_90c, low_90c, 3), 'sh_year': True},\n",
    "    {'name': 'Days warmer than 25', 'obj': Hotdays_simple('tasmax', 25), 'sh_year': True},\n",
    "    {'name': 'Days colder than than 10th pctle yearlong', 'obj': Colddays_simple('tasmin', cold_10y), 'sh_year': False},\n",
    "    {'name': 'Days colder than than 10th pctle mintemp', 'obj': Colddays_simple('tasmin', cold_10n), 'sh_year': False},\n",
    "    {'name': 'Days colder than than 13', 'obj': Colddays_simple('tasmin', 13), 'sh_year': False},\n",
    "    {'name': 'Cold wave (10th pctle yearlong)', 'obj': Tempwave_simple('tasmin', 6, cold_10y, False), 'sh_year': False},\n",
    "    {'name': 'Cold wave (10th pctle mintemp)', 'obj': Tempwave_simple('tasmin', 6, cold_10n, False), 'sh_year': False},\n",
    "]\n",
    "future_years = ((2025, 2032), (2032, 2040), (2040, 2050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "f5b286b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CanESM5+BCC-CSM2-MR: 60.3965\n",
      "BCC-CSM2-MR+KACE-1-0-G: 54.301\n",
      "MRI-ESM2-0+KIOST-ESM: 55.5586\n",
      "\n",
      "CanESM5: 83.2%\n",
      "BCC-CSM2-MR: 83.6%\n",
      "MRI-ESM2-0: 83.8%\n",
      "\n",
      "BCC-CSM2-MR: 83.3%\n",
      "KACE-1-0-G: 83.2%\n",
      "KIOST-ESM: 83.3%\n",
      "\n",
      "BCC-CSM2-MR: 5.5%\n",
      "KACE-1-0-G: 5.6%\n",
      "KIOST-ESM: 5.4%\n",
      "\n",
      "BCC-CSM2-MR: 83.3%\n",
      "KACE-1-0-G: 83.4%\n",
      "KIOST-ESM: 83.4%\n",
      "\n",
      "BCC-CSM2-MR: 9.5511\n",
      "KACE-1-0-G: 10.4714\n",
      "KIOST-ESM: 4.5116\n",
      "\n",
      "BCC-CSM2-MR: 0.4893\n",
      "KACE-1-0-G: 0.5134\n",
      "KIOST-ESM: 0.5004\n",
      "\n",
      "CanESM5+BCC-CSM2-MR: 55.4537\n",
      "BCC-CSM2-MR+KACE-1-0-G: 75.4389\n",
      "MRI-ESM2-0+KIOST-ESM: 77.5219\n",
      "\n",
      "CanESM5: 85.0%\n",
      "BCC-CSM2-MR: 84.8%\n",
      "MRI-ESM2-0: 85.1%\n",
      "\n",
      "BCC-CSM2-MR: 85.2%\n",
      "KACE-1-0-G: 84.9%\n",
      "KIOST-ESM: 84.7%\n",
      "\n",
      "BCC-CSM2-MR: 24.9%\n",
      "KACE-1-0-G: 5.0%\n",
      "KIOST-ESM: 5.0%\n",
      "\n",
      "BCC-CSM2-MR: 84.9%\n",
      "KACE-1-0-G: 85.1%\n",
      "KIOST-ESM: 85.1%\n",
      "\n",
      "BCC-CSM2-MR: 4.4813\n",
      "KACE-1-0-G: 10.4725\n",
      "KIOST-ESM: 8.4507\n",
      "\n",
      "BCC-CSM2-MR: 0.4976\n",
      "KACE-1-0-G: 0.4879\n",
      "KIOST-ESM: 0.5045\n",
      "\n",
      "CanESM5+BCC-CSM2-MR: 92.8026\n",
      "BCC-CSM2-MR+KACE-1-0-G: 94.3612\n",
      "MRI-ESM2-0+KIOST-ESM: 97.3208\n",
      "\n",
      "CanESM5: 87.6%\n",
      "BCC-CSM2-MR: 87.6%\n",
      "MRI-ESM2-0: 87.6%\n",
      "\n",
      "BCC-CSM2-MR: 87.5%\n",
      "KACE-1-0-G: 87.7%\n",
      "KIOST-ESM: 87.6%\n",
      "\n",
      "BCC-CSM2-MR: 4.2%\n",
      "KACE-1-0-G: 4.2%\n",
      "KIOST-ESM: 4.2%\n",
      "\n",
      "BCC-CSM2-MR: 87.5%\n",
      "KACE-1-0-G: 87.6%\n",
      "KIOST-ESM: 87.4%\n",
      "\n",
      "BCC-CSM2-MR: 8.5369\n",
      "KACE-1-0-G: 5.4561\n",
      "KIOST-ESM: 9.5118\n",
      "\n",
      "BCC-CSM2-MR: 0.4964\n",
      "KACE-1-0-G: 0.5099\n",
      "KIOST-ESM: 0.5148\n"
     ]
    }
   ],
   "source": [
    "loc = Location('Campinas_BRA', CAMPINAS_LATLON)\n",
    "with open('campinas_outputs.csv', 'w') as ofile:\n",
    "    ofile.write('City,Hazard,Year_range,Low_estimate,Middle_estimate,High_estimate\\n')\n",
    "    for fut_start, fut_end in future_years:\n",
    "        for haz in campinas_hazards:\n",
    "            est = Estimate(loc, haz['obj'], fut_start, fut_end, haz['sh_year'])\n",
    "            est.future_count()\n",
    "            res = list(est.estimate.values())\n",
    "            res.sort()\n",
    "            if haz['obj'].probmodel == 'binomial':\n",
    "                res = ['{0:.1f}%'.format(i / (fut_end - fut_start + 1) * 100) for i in res]\n",
    "            else:\n",
    "                res = ['{0:.1f}'.format(i / (fut_end - fut_start + 1)) for i in res]\n",
    "            ofile.write(','.join([loc.name, haz['name'], '{0} to {1}'.format(fut_start, fut_end), res[0], res[1], res[2]]))\n",
    "            ofile.write('\\n')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6c0106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba91dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7dd43ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFDL-CM4: min modeled value does not exceed observed 10th percentile  True\n",
      "GFDL-CM4: max modeled value does not exceed observed 90th percentile  True\n",
      "CanESM5: min modeled value does not exceed observed 10th percentile  True\n",
      "CanESM5: max modeled value does not exceed observed 90th percentile  True\n",
      "ACCESS-CM2: min modeled value does not exceed observed 10th percentile  True\n",
      "ACCESS-CM2: max modeled value does not exceed observed 90th percentile  True\n",
      "GFDL-CM4: min modeled value does not exceed observed 10th percentile  True\n",
      "GFDL-CM4: max modeled value does not exceed observed 90th percentile  True\n",
      "CanESM5: min modeled value does not exceed observed 10th percentile  True\n",
      "CanESM5: max modeled value does not exceed observed 90th percentile  True\n",
      "ACCESS-CM2: min modeled value does not exceed observed 10th percentile  True\n",
      "ACCESS-CM2: max modeled value does not exceed observed 90th percentile  True\n",
      "GFDL-CM4: min modeled value does not exceed observed 10th percentile  True\n",
      "GFDL-CM4: max modeled value does not exceed observed 90th percentile  True\n",
      "CanESM5: min modeled value does not exceed observed 10th percentile  True\n",
      "CanESM5: max modeled value does not exceed observed 90th percentile  True\n",
      "ACCESS-CM2: min modeled value does not exceed observed 10th percentile  True\n",
      "ACCESS-CM2: max modeled value does not exceed observed 90th percentile  True\n",
      "GFDL-CM4: min modeled value does not exceed observed 10th percentile  True\n",
      "GFDL-CM4: max modeled value does not exceed observed 90th percentile  True\n",
      "CanESM5: min modeled value does not exceed observed 10th percentile  False\n",
      "CanESM5: max modeled value does not exceed observed 90th percentile  True\n",
      "ACCESS-CM2: min modeled value does not exceed observed 10th percentile  False\n",
      "ACCESS-CM2: max modeled value does not exceed observed 90th percentile  True\n"
     ]
    }
   ],
   "source": [
    "for quarter in range(4):\n",
    "    obs_10 = np.percentile(quarters(hist_obs_tx, HIST_START, HIST_END)[quarter], 10)\n",
    "    obs_90 = np.percentile(quarters(hist_obs_tx, HIST_START, HIST_END)[quarter], 90)\n",
    "    for model in best_models_tx:\n",
    "        mod = quarters(hist_mods_tx[model] - 273.15, HIST_START, HIST_END)[quarter].flatten()\n",
    "        print('{0}: min modeled value does not exceed observed 10th percentile  {1}'.format(model, min(mod) <= obs_10))\n",
    "        print('{0}: max modeled value does not exceed observed 90th percentile  {1}'.format(model, max(mod) >= obs_90))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef9124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
