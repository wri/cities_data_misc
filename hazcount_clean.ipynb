{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abdba210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "%matplotlib inline\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "from scipy import stats\n",
    "import scipy\n",
    "\n",
    "import datetime, calendar\n",
    "import spei\n",
    "\n",
    "#ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "254785d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INFO = {'UKESM1-0-LL': 'HadAM',\n",
    " 'NorESM2-MM': 'CCM',\n",
    " 'NorESM2-LM': 'CCM',\n",
    " 'MRI-ESM2-0': 'UCLA GCM',\n",
    " 'MPI-ESM1-2-LR': 'ECMWF',\n",
    " 'MPI-ESM1-2-HR': 'ECMWF',\n",
    " 'MIROC6': 'MIROC',\n",
    " 'MIROC-ES2L': 'MIROC',\n",
    " 'KIOST-ESM': 'GFDL',\n",
    " 'KACE-1-0-G': 'HadAM',\n",
    " 'IPSL-CM6A-LR': 'IPSL',\n",
    " 'INM-CM5-0': 'INM',\n",
    " 'INM-CM4-8': 'INM',\n",
    " 'HadGEM3-GC31-MM': 'HadAM',\n",
    " 'HadGEM3-GC31-LL': 'HadAM',\n",
    " 'GFDL-ESM4': 'GFDL',\n",
    " 'GFDL-CM4_gr2': 'GFDL',\n",
    " 'GFDL-CM4': 'GFDL',\n",
    " 'FGOALS-g3': 'CCM',\n",
    " 'EC-Earth3-Veg-LR': 'ECMWF',\n",
    " 'EC-Earth3': 'ECMWF',\n",
    " 'CanESM5': 'CanAM',\n",
    " 'CNRM-ESM2-1': 'ECMWF',\n",
    " 'CNRM-CM6-1': 'ECMWF',\n",
    " 'CMCC-ESM2': 'CCM',\n",
    " 'CMCC-CM2-SR5': 'CCM',\n",
    " #'BCC-CSM2-MR': 'CCM',\n",
    " 'ACCESS-ESM1-5': 'HadAM',\n",
    " 'ACCESS-CM2': 'HadAM',\n",
    " 'TaiESM1': 'CCM',\n",
    "}\n",
    "\n",
    "EXCLUDED_MODELS = ['GFDL-CM4_gr2','ERA5']\n",
    "\n",
    "MODELS = [i for i in MODEL_INFO.keys() if not i in EXCLUDED_MODELS]\n",
    "\n",
    "\n",
    "CAMPINAS_LATLON = (-22.907104, -47.063240)  # Campinas\n",
    "\n",
    "HIST_START = 1980\n",
    "HIST_END = 2014\n",
    "\n",
    "PERCENTILE_STARTYEAR = 1980\n",
    "PERCENTILE_ENDYEAR = 2019\n",
    "\n",
    "NUM_BEST_MODELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66e61532",
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = {\n",
    "    'tasmax': {\n",
    "        'era_varname': 'maximum_2m_air_temperature',\n",
    "        'nex_transform': lambda x: x - 273.5,\n",
    "        'era_transform': lambda x: x - 273.5\n",
    "    },\n",
    "    'tasmin': {\n",
    "        'era_varname': 'minimum_2m_air_temperature',\n",
    "        'nex_transform': lambda x: x - 273.5,\n",
    "        'era_transform': lambda x: x - 273.5\n",
    "    },\n",
    "    'pr': {\n",
    "        'era_varname': 'total_precipitation',\n",
    "        'nex_transform': lambda x: x * 86400,\n",
    "        'era_transform': lambda x: x * 1000\n",
    "    },\n",
    "    'hurs': {\n",
    "        'era_varname': None,\n",
    "       'nex_transform': lambda x: x,\n",
    "        'era_transform': lambda x: x\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9af88e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calendardate_percentiles(nex_varname, q, latlon, sh_hem=False):\n",
    "    hist_start = PERCENTILE_STARTYEAR\n",
    "    hist_end = PERCENTILE_ENDYEAR\n",
    "    allyears = []\n",
    "    for year in range(hist_start, hist_end):\n",
    "        allyears.append(get_observed_gee(nex_varname, latlon, start_year=year, end_year=year, southern_hem=False))\n",
    "    if not sh_hem:\n",
    "        return np.percentile(np.vstack(allyears), q, axis=0)\n",
    "    else:\n",
    "        res = np.percentile(np.vstack(allyears), q, axis=0)\n",
    "        return np.concatenate([res[152:], res[:152]])\n",
    "\n",
    "def wholeyear_percentile(nex_varname, q, latlon):\n",
    "    if not nex_varname == 'ari':\n",
    "        hist_start = PERCENTILE_STARTYEAR\n",
    "        hist_end = PERCENTILE_ENDYEAR\n",
    "        allyears = []\n",
    "        for year in range(hist_start, hist_end):\n",
    "            allyears.append(get_observed_gee(nex_varname, latlon, start_year=year, end_year=year, southern_hem=False))\n",
    "        return np.percentile(np.concatenate(allyears).flatten(), q)\n",
    "    else:\n",
    "        hist_start = PERCENTILE_STARTYEAR\n",
    "        hist_end = PERCENTILE_ENDYEAR\n",
    "        allyears = []\n",
    "        for year in range(hist_start, hist_end):\n",
    "            allyears.append(get_observed_gee('pr', latlon, start_year=year, end_year=year, southern_hem=False))\n",
    "        ari_data = ari(np.concatenate(allyears).flatten())\n",
    "        return np.percentile(ari_data, 95)\n",
    "\n",
    "def yearextreme_percentile(nex_varname, q, latlon, wantmax):\n",
    "    hist_start = PERCENTILE_STARTYEAR\n",
    "    hist_end = PERCENTILE_ENDYEAR\n",
    "    allyears = []\n",
    "    for year in range(hist_start, hist_end):\n",
    "        allyears.append(get_observed_gee(nex_varname, latlon, start_year=year, end_year=year, southern_hem=False))\n",
    "    return np.percentile(np.array(allyears), q)\n",
    "\n",
    "def d2j(datestring):\n",
    "    d = datetime.date.fromisoformat(datestring)\n",
    "    jday = d.timetuple().tm_yday\n",
    "    if calendar.isleap(d.year) and jday > 59:\n",
    "        jday -= 1\n",
    "    return jday\n",
    "\n",
    "def removeLeapDays(arr, start_year, end_year, southern_hem):\n",
    "    indices_to_remove = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        if calendar.isleap(year):\n",
    "            indices_to_remove.append(((year-start_year) * 365) + [0,183][int(southern_hem)] + len(indices_to_remove) + 59)\n",
    "    return np.delete(arr, indices_to_remove)\n",
    "\n",
    "def get_rmsd(d1, d2):\n",
    "    c1 = seasonal_means(d1)\n",
    "    c2 = seasonal_means(d2)\n",
    "    return np.sqrt(np.mean(np.sum((c1 - c2)**2)))\n",
    "\n",
    "def count_runs(tf_array, min_runsize):\n",
    "    falses = np.zeros(tf_array.shape[0]).reshape((tf_array.shape[0],1))\n",
    "    extended_a = np.concatenate([[0], tf_array, [0]])\n",
    "    df = np.diff(extended_a)\n",
    "    starts = np.nonzero(df == 1)[0]\n",
    "    ends = np.nonzero(df == -1)[0]\n",
    "    count = 0\n",
    "    for idx in range(starts.size):\n",
    "        if ends[idx] - starts[idx] >= min_runsize:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def longest_run(tf_array):\n",
    "    if np.sum(tf_array) == 0:\n",
    "        return 0\n",
    "    falses = np.zeros(tf_array.shape[0]).reshape((tf_array.shape[0],1))\n",
    "    extended_a = np.concatenate([[0], tf_array, [0]])\n",
    "    df = np.diff(extended_a)\n",
    "    starts = np.nonzero(df == 1)[0]\n",
    "    ends = np.nonzero(df == -1)[0]\n",
    "    durations = ends - starts\n",
    "    return max(durations)\n",
    "    \n",
    "def quarters(d, start_year, end_year, southern_hem=False):\n",
    "    #Takes multi-year array and returns data reorganized into quarters\n",
    "    q2 = []  # 60-151\n",
    "    q3 = []  # 152-243\n",
    "    q4 = []  # 244-334\n",
    "    q1 = []  # 335-59\n",
    "    if not southern_hem:\n",
    "        jan1_idx = 365\n",
    "        for year in range(start_year, end_year):\n",
    "            tmp = np.concatenate((d[jan1_idx - 365 : jan1_idx - 365 + 60], d[jan1_idx + 335 : jan1_idx + 365]), axis=0)\n",
    "            q1.append(tmp)\n",
    "            q2.append(d[jan1_idx + 60 : jan1_idx + 152])\n",
    "            q3.append(d[jan1_idx + 152 : jan1_idx + 244])\n",
    "            q4.append(d[jan1_idx + 244 : jan1_idx + 335])\n",
    "\n",
    "            jan1_idx += 365 + [0, 0][int(False and calendar.isleap(year))]\n",
    "        mam_res = np.vstack(q2)\n",
    "        jja_res = np.vstack(q3)\n",
    "        son_res = np.vstack(q4)\n",
    "        djf_res = np.vstack(q1)\n",
    "    else:\n",
    "        jul1_idx = 365\n",
    "        for year in range(start_year, end_year):\n",
    "            tmp = np.concatenate((d[jul1_idx - 365 : jul1_idx - 365 + 60], d[jul1_idx + 335 : jul1_idx + 365]), axis=0)\n",
    "            q3.append(tmp)\n",
    "            q4.append(d[jul1_idx + 60 : jul1_idx + 152])\n",
    "            q1.append(d[jul1_idx + 152 : jul1_idx + 244])\n",
    "            q2.append(d[jul1_idx + 244 : jul1_idx + 335])\n",
    "\n",
    "            jul1_idx += 365 + [0, 0][int(False and calendar.isleap(year))]\n",
    "        mam_res = np.vstack(q4)\n",
    "        jja_res = np.vstack(q1)\n",
    "        son_res = np.vstack(q2)\n",
    "        djf_res = np.vstack(q3)\n",
    "    return mam_res, jja_res, son_res, djf_res\n",
    "    \n",
    "def seasonal_means(d):\n",
    "    q = quarters(d, HIST_START, HIST_END)\n",
    "    return np.array([np.mean(q[0], axis=1), np.mean(q[1], axis=1), np.mean(q[2], axis=1), np.mean(q[3], axis=1)])\n",
    "\n",
    "def calibration_function(hist_obs, hist_mod):\n",
    "# Calibration functions are P-P plots of historical and modeled values\n",
    "\n",
    "    source = np.sort(hist_obs.flatten())\n",
    "    target= np.sort(hist_mod.flatten())\n",
    "   \n",
    "    if (np.max(source) == 0 and np.min(source) == 0):\n",
    "        return np.arange(0, target.size) / target.size\n",
    "    if (np.max(target) == 0 and np.min(target) == 0):\n",
    "        return np.arange(0, source.size) / source.size\n",
    "    new_indices = []\n",
    "\n",
    "    for target_idx, target_value in enumerate(target):\n",
    "        if target_idx < len(source):\n",
    "            source_value = source[target_idx]\n",
    "            if source_value > target[-1]:\n",
    "                new_indices.append(target.size - 1)\n",
    "            else:\n",
    "                new_indices.append(np.argmax(target >= source_value))\n",
    "    return np.array(new_indices) / source.size\n",
    "\n",
    "def calibrate_component(uncalibrated_data, calibration_fxn):\n",
    "    N = len(uncalibrated_data)\n",
    "    unsorted_uncalib = [(i, idx) for idx, i in enumerate(uncalibrated_data)]\n",
    "    sorted_uncalib = sorted(unsorted_uncalib)\n",
    "    result = [0] * N\n",
    "    for j in range(N):\n",
    "        X_j = j / (N + 1)\n",
    "        Y_jprime = calibration_fxn[math.floor(X_j * len(calibration_fxn))]\n",
    "        jprime = math.floor(Y_jprime * (N + 1))\n",
    "        result[sorted_uncalib[j][1]] = sorted_uncalib[min(len(sorted_uncalib)-1, jprime)][0]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def calibrate(uncalibrated_data, calibration_fxn):\n",
    "    mam = []\n",
    "    jja = []\n",
    "    son = []\n",
    "    djf = []\n",
    "    mam_idx = []\n",
    "    jja_idx = []\n",
    "    son_idx = []\n",
    "    djf_idx = []\n",
    "    for idx, i in enumerate(uncalibrated_data):\n",
    "        if idx % 365 >= 60 and idx % 365 < 152:\n",
    "            mam.append(uncalibrated_data[idx])\n",
    "            mam_idx.append(idx)\n",
    "        elif idx % 365 >= 152 and idx % 365 < 244:\n",
    "            jja.append(uncalibrated_data[idx])\n",
    "            jja_idx.append(idx)\n",
    "        elif idx % 365 >= 244 and idx % 365 < 335:\n",
    "            son.append(uncalibrated_data[idx])\n",
    "            son_idx.append(idx)\n",
    "        else:\n",
    "            djf.append(uncalibrated_data[idx])\n",
    "            djf_idx.append(idx)\n",
    "    \n",
    "    mam_calib = calibrate_component(np.array(mam), calibration_fxn[0])\n",
    "    jja_calib = calibrate_component(np.array(jja), calibration_fxn[1])\n",
    "    son_calib = calibrate_component(np.array(son), calibration_fxn[2])\n",
    "    djf_calib = calibrate_component(np.array(djf), calibration_fxn[3])\n",
    "    \n",
    "    result = [0] * len(uncalibrated_data)\n",
    "    for i in range(len(mam_idx)):\n",
    "        result[mam_idx[i]] = mam_calib[i]\n",
    "    for i in range(len(jja_idx)):\n",
    "        result[jja_idx[i]] = jja_calib[i]\n",
    "    for i in range(len(son_idx)):\n",
    "        result[son_idx[i]] = son_calib[i]\n",
    "    for i in range(len(djf_idx)):\n",
    "        result[djf_idx[i]] = djf_calib[i]\n",
    "\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5dea0425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ari(yeardata):\n",
    "    ARI_WEIGHTS = np.array([\n",
    "        0.013499274414000246,\n",
    "        0.01837401239683367,\n",
    "        0.026458577851440485,\n",
    "        0.041341527892875755,\n",
    "        0.07349604958733467,\n",
    "        0.16536611157150302,\n",
    "        0.6614644462860121\n",
    "    ])\n",
    "    def ari_7day(sevendayrain):\n",
    "        return np.dot(sevendayrain, ARI_WEIGHTS)\n",
    "    \n",
    "    res = []\n",
    "    for start_idx in range(yeardata.size-7):\n",
    "        res.append(ari_7day(yeardata[start_idx:start_idx+7]))\n",
    "    return res\n",
    "\n",
    "def wetbulbtemp(T, RH):\n",
    "# JA Knox et al. 2017. Two simple and accurate approximations for wet-bulb\n",
    "# temperature in moist conditions, with forecasting applications. Bull. Am.\n",
    "# Meteorol. Soc. 98(9): 1897-1906. doi:10.1175/BAMS-D-16-0246.1\n",
    "    T = T.astype(np.float64)\n",
    "    rh_percent = RH.astype(np.float64)\n",
    "    return T * np.arctan(0.151977 * np.sqrt(rh_percent + 8.313659)) + np.arctan(T + rh_percent) - np.arctan(rh_percent - 1.676331) + ((0.00391838 * ((rh_percent)**(3/2))) * np.arctan(0.023101 * rh_percent)) - 4.686035        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "05aa86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hazard:\n",
    "    def get_expectedval(self, latlon, start_year, end_year, datasets, calib_fxns):\n",
    "        southern_hem = int(latlon[0] < 0)\n",
    "        fut_mod = {}\n",
    "        varnames = self.varname.split('+')\n",
    "        for varname in varnames:\n",
    "            for model in calib_fxns[varname].keys():\n",
    "                ds = datasets[varname][model]\n",
    "\n",
    "                fut_mod[(varname, model)] = ds\n",
    "        best_models = []\n",
    "        for idx in range(NUM_BEST_MODELS):\n",
    "            modelplus = '+'.join([list(calib_fxns[varname].keys())[idx] for varname in varnames])\n",
    "            best_models.append(modelplus)\n",
    "\n",
    "        para_res = {}\n",
    "        numbins = end_year - start_year + 1\n",
    "        for modelplus in best_models:\n",
    "            calib_data = []\n",
    "            for idx, varname in enumerate(varnames):\n",
    "                model = modelplus.split('+')[idx]\n",
    "                calib_data.append(np.array(calibrate(fut_mod[(varname, model)], calib_fxns[varname][model])))\n",
    "            countdist = self.val_dist([cd[[0,152][int(not southern_hem)]:[len(cd),-213][int(not southern_hem)]] for cd in calib_data])\n",
    "        \n",
    "            observed_vals = np.array(list(countdist.keys()))\n",
    "            minval = observed_vals[0] - 1\n",
    "            maxval = observed_vals[-1] + 1\n",
    "            D = (maxval - minval) / (numbins - 1)\n",
    "            for i in range(numbins):\n",
    "                centerval = minval + (i * D)\n",
    "                countdist[centerval] = np.sum(observed_vals >= minval + ((i - 0.5) * D) * (observed_vals < minval + ((i + 0.5) * D)))\n",
    "            alpha = np.array(list(countdist.values())) + 0.5\n",
    "            res = []\n",
    "            for i in range(10000):\n",
    "                dirich_samp = np.random.dirichlet(alpha, 1)\n",
    "                mult_samp = np.random.multinomial(end_year - start_year + 1, dirich_samp[0], 1)[0]\n",
    "                res.append(sum([list(countdist.keys())[j] * mult_samp[j] for j in range(len(list(countdist.keys())))]) / (end_year - start_year + 1))\n",
    "            res = np.array(res)\n",
    "            para_res[modelplus] = res\n",
    "\n",
    "        return {modelplus:[np.mean(para_res[modelplus])- (1.96 * np.std(para_res[modelplus])), np.mean(para_res[modelplus]), np.mean(para_res[modelplus]) + (1.96 * np.std(para_res[modelplus]))] for modelplus in best_models}\n",
    "    \n",
    "    def get_exceedanceprob(self, latlon, start_year, end_year, datasets, calib_fxns):\n",
    "        southern_hem = int(latlon[0] < 0)\n",
    "        fut_mod = {}\n",
    "        varnames = self.varname.split('+')\n",
    "        for varname in varnames:\n",
    "            for model in calib_fxns[varname].keys():\n",
    "                ds = datasets[varname][model]\n",
    "                fut_mod[(varname, model)] = ds\n",
    "        best_models = []\n",
    "        for idx in range(NUM_BEST_MODELS):\n",
    "            modelplus = '+'.join([list(calib_fxns[varname].keys())[idx] for varname in varnames])\n",
    "            best_models.append(modelplus)\n",
    "\n",
    "        prob_res = {}\n",
    "        for modelplus in best_models:\n",
    "            calib_data = []\n",
    "            for idx, varname in enumerate(varnames):\n",
    "                model = modelplus.split('+')[idx]\n",
    "                calib_data.append(np.array(calibrate(fut_mod[(varname, model)], calib_fxns[varname][model])))\n",
    "            countdist = self.val_dist([cd[[0,152][int(not southern_hem)]:[len(cd),-213][int(not southern_hem)]] for cd in calib_data])\n",
    "        \n",
    "            count = sum([countdist[val] * int((val >= self.extremeval and self.exceed_is_gte) or (val <= self.extremeval and not self.exceed_is_gte)) for val in countdist])\n",
    "            N = end_year - start_year + 1\n",
    "            if self.probmodel == 'Poisson':\n",
    "                meanprob = (count + 0.5) / N\n",
    "                stdprob = np.sqrt((2 * count) + 1) / N\n",
    "            else: \n",
    "                meanprob = ((count + 0.5) * N / (N + 1)) / N\n",
    "                stdprob = (np.sqrt((N * (count + 0.5) * (N - count + 0.5) * ((2 * N) + 1)) / ((N + 2) * (N + 1) * (N + 1)))) / N\n",
    "            prob_res[modelplus] = [meanprob - (1.96 * stdprob), meanprob, meanprob + (1.96 * stdprob)]\n",
    "       \n",
    "        return prob_res\n",
    "\n",
    "class ARIDays(Hazard):\n",
    "    def __init__(self, hazname, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'pr'\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')   \n",
    "        byyear = data.reshape(data.size // 365, 365)\n",
    "        \n",
    "        vals = apply_along_axis(ari, data, axis=1)\n",
    "        \n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "    \n",
    "class WetbulbDays(Hazard):\n",
    "    def __init__(self, hazname, wbt_threshold, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'tasmax+hurs'\n",
    "        self.wbt_threshold = wbt_threshold\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "    \n",
    "    def val_dist(self, datalist):\n",
    "        data_t = datalist[0]\n",
    "        data_h = datalist[1]\n",
    "        data = wetbulbtemp(data_t, data_h)\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')\n",
    "        byyear = data.reshape(data.size//365, 365)\n",
    "        vals = np.sum(byyear >= self.wbt_threshold, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "\n",
    "class DroughtSPIDays(Hazard):\n",
    "    def __init__(self, hazname, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'pr'\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "    \n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')\n",
    "        \n",
    "        t=pd.date_range(start='1980-01-01', end='{0}-12-31'.format(1980 + (data.size//365) - 1), freq='D')\n",
    "        t = t[~((t.month == 2) & (t.day == 29))]\n",
    "        \n",
    "        droughtdays = spei.spi(pd.Series(data, index=t)).to_numpy()\n",
    "        byyear = droughtdays.reshape(data.size // 365, 365)\n",
    "        \n",
    "        vals = np.sum(byyear <= -2, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "\n",
    "class Tempwave_count(Hazard):\n",
    "    def __init__(self, hazname, varname, min_duration, threshold, tf_gte, extremeval):\n",
    "        if type(threshold) == np.ndarray and threshold.size % 365 != 0:\n",
    "            raise Exception('Comparison array length is not an integer multiple of 365')\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.tf_gte = tf_gte\n",
    "        self.min_duration = min_duration\n",
    "        self.threshold = threshold  # May be scalar or 365-long array\n",
    "        self.probmodel = 'Poisson'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "    def tf_array(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if self.tf_gte:\n",
    "            return data >= self.threshold\n",
    "        else:\n",
    "            return data <= self.threshold\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        tfarray = self.tf_array(datalist)\n",
    "        tfarray = tfarray.reshape(tfarray.size//365, 365)\n",
    "        vals = np.apply_along_axis(count_runs, 1, tfarray, self.min_duration)\n",
    "        \n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "    \n",
    "class Tempwave_maxduration(Hazard):\n",
    "    def __init__(self, hazname, varname, threshold, tf_gte, extremeval):\n",
    "        if type(threshold) == np.ndarray and threshold.size % 365 != 0:\n",
    "            raise Exception('Comparison array length is not an integer multiple of 365')\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.tf_gte = tf_gte\n",
    "        self.min_duration = min_duration\n",
    "        self.threshold = threshold  # May be scalar or 365-long array\n",
    "        self.probmodel = 'Poisson'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "    def tf_array(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if self.tf_gte:\n",
    "            return data >= self.threshold\n",
    "        else:\n",
    "            return data <= self.threshold\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        tfarray = self.tf_array(datalist)\n",
    "        tfarray = tfarray.reshape(tfarray.size//365, 365)\n",
    "        vals = np.apply_along_axis(longest_run, 1, tfarray)\n",
    "        \n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "\n",
    "class Tempwave_highlow_count(Hazard):\n",
    "    def __init__(self, hazname, hightemp, lowtemp, min_duration, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'tasmax+tasmin'\n",
    "        self.min_duration = min_duration\n",
    "        self.hightemp = hightemp\n",
    "        self.lowtemp = lowtemp\n",
    "        self.probmodel = 'Poisson'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "    def tf_array(self, datalist):\n",
    "        data_tx = datalist[0]\n",
    "        data_tn = datalist[1]\n",
    "        if type(self.hightemp) in (float, int, np.float64, np.int32):\n",
    "            high_threshold = self.hightemp\n",
    "        else:   # type is np array\n",
    "            high_threshold = np.array([])\n",
    "            while high_threshold.size < data_tx.size:\n",
    "                high_threshold = np.concatenate([high_threshold, self.hightemp])\n",
    "        if type(self.lowtemp) in (float, int, np.float64, np.int32):\n",
    "            low_threshold = self.lowtemp\n",
    "        else:   # type is np array\n",
    "            low_threshold = np.array([])\n",
    "            while low_threshold.size < data_tn.size:\n",
    "                low_threshold = np.concatenate([low_threshold, self.lowtemp])\n",
    "        tf_array_tx = data_tx >= high_threshold\n",
    "        tf_array_tn = data_tn >= low_threshold\n",
    "        return tf_array_tx * tf_array_tn\n",
    "    \n",
    "    def val_dist(self, datalist):\n",
    "        tfarray = self.tf_array(datalist)\n",
    "        tfarray = tfarray.reshape(tfarray.size//365, 365)\n",
    "        vals = np.apply_along_axis(count_runs, 1, tfarray, self.min_duration)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "    \n",
    "class Tempwave_highlow_duration(Hazard):\n",
    "    def __init__(self, hazname, hightemp, lowtemp, min_duration, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'tasmax+tasmin'\n",
    "        self.min_duration = min_duration\n",
    "        self.hightemp = hightemp\n",
    "        self.lowtemp = lowtemp\n",
    "        self.probmodel = 'Poisson'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "        \n",
    "    def tf_array(self, datalist):\n",
    "        data_tx = datalist[0]\n",
    "        data_tn = datalist[1]\n",
    "        if type(self.hightemp) in (float, int, np.float64, np.int32):\n",
    "            high_threshold = self.hightemp\n",
    "        else:   # type is np array\n",
    "            high_threshold = np.array([])\n",
    "            while high_threshold.size < data_tx.size:\n",
    "                high_threshold = np.concatenate([high_threshold, self.hightemp])\n",
    "        if type(self.lowtemp) in (float, int, np.float64, np.int32):\n",
    "            low_threshold = self.lowtemp\n",
    "        else:   # type is np array\n",
    "            low_threshold = np.array([])\n",
    "            while low_threshold.size < data_tn.size:\n",
    "                low_threshold = np.concatenate([low_threshold, self.lowtemp])\n",
    "        tf_array_tx = data_tx >= high_threshold\n",
    "        tf_array_tn = data_tn >= low_threshold\n",
    "        return tf_array_tx * tf_array_tn\n",
    "    \n",
    "    def val_dist(self, datalist):\n",
    "        tfarray = self.tf_array(datalist)\n",
    "        tfarray = tfarray.reshape(tfarray.size//365, 365)\n",
    "        vals = np.apply_along_axis(longest_run, 1, tfarray)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "\n",
    "class ThresholdDays(Hazard):\n",
    "    def __init__(self, hazname, varname, var_threshold, want_max, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.var_threshold = var_threshold\n",
    "        self.want_max = want_max\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')   \n",
    "        byyear = data.reshape(data.size // 365, 365)\n",
    "        \n",
    "        if self.want_max:\n",
    "            vals = np.sum(byyear >= self.var_threshold, axis=1)\n",
    "        else:\n",
    "            vals = np.sum(byyear <= self.var_threshold, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "        \n",
    "class Dryduration_seasonal_count(Hazard):\n",
    "    def __init__(self, hazname, startdate, enddate, southern_hem, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'pr'\n",
    "        self.startdate = startdate\n",
    "        self.enddate = enddate\n",
    "        self.southern_hem = southern_hem\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "    def tf_array(self, datalist):\n",
    "        data = datalist[0] == 0\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')   \n",
    "        byyear = data.reshape(data.size//365, 365)\n",
    "        \n",
    "        start_jday = d2j('1999-{0}'.format(self.startdate)) - [0, 182][int(self.southern_hem)]\n",
    "        end_jday = d2j('1999-{0}'.format(self.enddate)) - [0, 182][int(self.southern_hem)]\n",
    "        if end_jday < start_jday:\n",
    "            end_jday += 365\n",
    "        inseason_onerow = [((i >= start_jday)and(i <= end_jday)) for i in range(365)]\n",
    "        inseason = np.array(inseason_onerow * (data.size//365))\n",
    "        inseason = inseason.reshape(data.size//365, 365)\n",
    "        return byyear * inseason\n",
    "\n",
    "    def val_dist(self, datalist): \n",
    "        byyear = self.tf_array(datalist)\n",
    "        vals = np.apply_along_axis(longest_run, 1, byyear)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "    \n",
    "class Drycount_seasonal(Hazard):\n",
    "    def __init__(self, hazname, startdate, enddate, southern_hem, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = 'pr'\n",
    "        self.startdate = startdate\n",
    "        self.enddate = enddate\n",
    "        self.southern_hem = southern_hem\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "    def tf_array(self, datalist):\n",
    "        \n",
    "        data = datalist[0] == 0\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')   \n",
    "        byyear = data.reshape(data.size//365, 365)\n",
    "        \n",
    "        start_jday = d2j('1999-{0}'.format(self.startdate)) - [0, 182][int(self.southern_hem)]\n",
    "        end_jday = d2j('1999-{0}'.format(self.enddate)) - [0, 182][int(self.southern_hem)]\n",
    "        if end_jday < start_jday:\n",
    "            end_jday += 365\n",
    "        inseason_onerow = [((i >= start_jday)and(i <= end_jday)) for i in range(365)]\n",
    "        inseason = np.array(inseason_onerow * data.size//365)\n",
    "        return (byyear * inseason) == 0\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        tfarray = self.tf_array(datalist)\n",
    "        vals = np.apply_along_axis(count_runs, 1, tfarray, self.min_duration)\n",
    "        \n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "\n",
    "\n",
    "class Annual_val(Hazard):\n",
    "    def __init__(self, hazname, varname, aggtype, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.aggtype = aggtype\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        byyear = data.reshape(data.size//365, 365)\n",
    "        if self.aggtype == 'sum':\n",
    "            vals = np.sum(byyear, axis=1)\n",
    "        elif self.aggtype == 'mean':\n",
    "            vals = np.mean(byyear, axis=1)\n",
    "        elif self.aggtype == 'max':\n",
    "            vals = np.max(byyear, axis=1)\n",
    "        elif self.aggtype == 'min':\n",
    "            vals = np.min(byyear, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "\n",
    "class Seasonal_val(Hazard):\n",
    "    def __init__(self, hazname, varname, aggtype, startdate, enddate, southern_hem, extremeval):\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.aggtype = aggtype\n",
    "        self.startdate = startdate\n",
    "        self.enddate = enddate\n",
    "        self.southern_hem = southern_hem\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        self.extremeval = extremeval\n",
    "        \n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        byyear = data.reshape(data.size//365, 365)\n",
    "        start_jday = d2j('1999-{0}'.format(self.startdate)) - [0, 182][int(self.southern_hem)]\n",
    "        end_jday = d2j('1999-{0}'.format(self.enddate)) - [0, 182][int(self.southern_hem)]\n",
    "        if end_jday < start_jday:\n",
    "            end_jday += 365\n",
    "        inseason_onerow = [((i >= start_jday)and(i <= end_jday)) for i in range(365)]\n",
    "        inseason = np.array([inseason_onerow]*(data.size//365))\n",
    "        byyear = byyear * inseason\n",
    "        if self.aggtype == 'sum':\n",
    "            vals = np.sum(byyear, axis=1)\n",
    "        elif self.aggtype == 'mean':\n",
    "            vals = np.mean(byyear, axis=1)\n",
    "        elif self.aggtype == 'max':\n",
    "            vals = np.max(byyear, axis=1)\n",
    "        elif self.aggtype == 'min':\n",
    "            vals = np.min(byyear, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        print(vals)\n",
    "        return result_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b21c38fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observed_gee(varname, latlon, start_year, end_year, southern_hem=False):\n",
    "    def relhum(T, Tdp):\n",
    "        T = T.astype('float64')\n",
    "        Tdp = Tdp.astype('float64')\n",
    "        numerator = np.exp(17.625 * Tdp / (243.04 + Tdp))\n",
    "        denominator = np.exp(17.625 * T / (243.04 + T))\n",
    "        return 100 * numerator / denominator\n",
    "\n",
    "    def get_eradata(varname, southern_hem=False):\n",
    "        # Return numpy array in correct units, leapdays removed\n",
    "        dataset = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\n",
    "        gee_geom = ee.Geometry.Point((latlon[1], latlon[0]))\n",
    "        data_vars = dataset.select(varname).filter(ee.Filter.date('{0}-01-01'.format(start_year), '{0}-01-01'.format(end_year+ 1)))\n",
    "        success = False\n",
    "        while not success:\n",
    "            try:\n",
    "                d = data_vars.getRegion(gee_geom, 2500, 'epsg:4326').getInfo()\n",
    "                success = True\n",
    "            except:\n",
    "                print('\\nRetrying')\n",
    "        result = [i[4] for i in d[1:]]\n",
    "        return np.array(result)\n",
    "    \n",
    "    if varname == 'hurs':\n",
    "        success = False\n",
    "        era_dewpoint = get_eradata('dewpoint_2m_temperature')-273.15\n",
    "        era_maxtemp = get_eradata('maximum_2m_air_temperature')-273.15\n",
    "        hist_obs = relhum(era_maxtemp, era_dewpoint)\n",
    "    elif varname == 'pr':\n",
    "        hist_obs = get_eradata('total_precipitation') * 1000\n",
    "    elif varname == 'tasmax':\n",
    "        hist_obs = get_eradata('maximum_2m_air_temperature')-273.15\n",
    "    else:    # varname == 'tasmin'\n",
    "        hist_obs = get_eradata('minimum_2m_air_temperature')-273.15\n",
    "    return removeLeapDays(hist_obs, start_year, end_year, southern_hem)\n",
    "\n",
    "def get_modeled_gee(varname, scenario, model, lat, lon, southern_hem, start_year, end_year):\n",
    "    # Return numpy array in correct units, leapdays removed\n",
    "    dataset = ee.ImageCollection('NASA/GDDP-CMIP6').filter(ee.Filter.eq('model', model)).filter(ee.Filter.eq('scenario', scenario))\n",
    "    gee_geom = ee.Geometry.Point((lon, lat))\n",
    "    if start_year >= 2015:\n",
    "        if southern_hem:\n",
    "            data_vars = dataset.select(varname).filter(ee.Filter.date('{0}-07-01'.format(start_year-1), '{0}-07-01'.format(end_year)))\n",
    "        else:\n",
    "            data_vars = dataset.select(varname).filter(ee.Filter.date('{0}-01-01'.format(start_year), '{0}-01-01'.format(end_year+ 1)))\n",
    "        result = [i[4] for i in data_vars.getRegion(gee_geom, 2500, 'epsg:4326').getInfo()[1:]]\n",
    "    else:\n",
    "        hist_dataset = ee.ImageCollection('NASA/GDDP-CMIP6').filter(ee.Filter.eq('model', model))\n",
    "        if southern_hem:\n",
    "            hist_part = hist_dataset.select(varname).filter(ee.Filter.eq('scenario', 'historical')).filter(ee.Filter.date('{0}-07-01'.format(start_year-1), '2015-01-01'))\n",
    "            if end_year >= 2015:\n",
    "                ssp_part = dataset.select(varname).filter(ee.Filter.eq('scenario', scenario)).filter(ee.Filter.date('2015-01-01', '{0}-07-01'.format(end_year)))\n",
    "        else:\n",
    "            hist_part = hist_dataset.select(varname).filter(ee.Filter.eq('scenario', 'historical')).filter(ee.Filter.date('{0}-01-01'.format(start_year), '2015-01-01'))\n",
    "            if end_year >= 2015:\n",
    "                ssp_part = dataset.select(varname).filter(ee.Filter.eq('scenario', scenario)).filter(ee.Filter.date('2015-01-01'.format(start_year-1), '{0}-01-01'.format(end_year+ 1)))\n",
    "        hist_result = [i[4] for i in hist_part.getRegion(gee_geom, 2500, 'epsg:4326').getInfo()[1:]]\n",
    "        if end_year >= 2015:\n",
    "            ssp_result = [i[4] for i in ssp_part.getRegion(gee_geom, 2500, 'epsg:4326').getInfo()[1:]]\n",
    "        else:\n",
    "            ssp_result = []\n",
    "        result = hist_result + ssp_result\n",
    "    d =  VARIABLES[varname]['nex_transform'](np.array(result))\n",
    "    return removeLeapDays(d, start_year, end_year, southern_hem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a2955afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calibfxns(varname, latlon, start_year, end_year):\n",
    "    hist_obs = get_observed_gee(varname, latlon, start_year, end_year, southern_hem=False)\n",
    "    hist_mods = {}\n",
    "    rmsds = []\n",
    "    for model in MODELS:\n",
    "        hist_mod = get_modeled_gee(varname, 'historical', model, latlon[0], latlon[1], False, start_year, end_year)\n",
    "        hist_mods[model] = hist_mod\n",
    "        rmsds.append((get_rmsd(hist_obs, hist_mod), model))\n",
    "    rmsds.sort()\n",
    "    best_models = []\n",
    "    families = []\n",
    "    idx = 0\n",
    "    while len(best_models) < 3:\n",
    "        if not MODEL_INFO[rmsds[idx][1]] in families:\n",
    "            best_models.append(rmsds[idx][1])\n",
    "            families.append(MODEL_INFO[rmsds[idx][1]])\n",
    "        idx += 1\n",
    "\n",
    "# Get calibration functions\n",
    "    calib_fxns = {}\n",
    "    for model in best_models:\n",
    "        o_quarters = quarters(hist_obs, HIST_START, HIST_END)\n",
    "        m_quarters = quarters(hist_mods[model], HIST_START, HIST_END)\n",
    "        calib_fxns[model] = [calibration_function(o_quarters[i].flatten(), m_quarters[i].flatten()) for i in range(4)]\n",
    "    return calib_fxns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9066e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata(varnames, scenario, lat, lon, start_year, end_year, best_models):\n",
    "    return {varname: {model: get_modeled_gee(varname, scenario, model, lat, lon, lat < 0, start_year, end_year) for model in best_models[varname]} for varname in varnames}\n",
    "\n",
    "def do_locationhazard(hazard, datasets, latlon, scenario, start_year, end_year, calib_fxns):\n",
    "    lat, lon = latlon\n",
    "    varnames = hazard.varname.split('+')\n",
    "    return lat, lon, hazard.hazname, scenario, '{0}-{1}'.format(start_year, end_year), hazard.get_expectedval(latlon, start_year, end_year, datasets, calib_fxns), hazard.get_exceedanceprob(latlon, start_year, end_year, datasets, calib_fxns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d5659f8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "high_90c = calendardate_percentiles('tasmax', 90, CAMPINAS_LATLON, sh_hem=True)\n",
    "low_90c = calendardate_percentiles('tasmin', 90, CAMPINAS_LATLON, sh_hem=True)\n",
    "hot_95y = wholeyear_percentile('tasmax', 95, CAMPINAS_LATLON)\n",
    "#cold_5y = wholeyear_percentile('tasmin', 5, CAMPINAS_LATLON)\n",
    "#cold_10y = wholeyear_percentile('tasmin', 10, CAMPINAS_LATLON)\n",
    "#ari_95y = wholeyear_percentile('ari', 95, CAMPINAS_LATLON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f52c31fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980 2014\n",
      "1980 2014\n",
      "1980 2014\n",
      "1980 2014\n"
     ]
    }
   ],
   "source": [
    "VARNAMES = ['tasmax', 'tasmin', 'pr']\n",
    "lat, lon = CAMPINAS_LATLON\n",
    "calib_fxns = {varname: get_calibfxns(varname, (lat, lon), HIST_START, HIST_END) for varname in VARNAMES}\n",
    "best_models = {varname: list(calib_fxns[varname].keys()) for varname in VARNAMES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2f1fa2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.53 s\n",
      "Wall time: 21.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lat, lon = CAMPINAS_LATLON\n",
    "HAZARDS = [\n",
    "    Tempwave_highlow_count('Heat wave', high_90c, low_90c, 3, 1),\n",
    "    Tempwave_highlow_duration('Heat wave', high_90c, low_90c, 3, 5),\n",
    "    ThresholdDays('Days warmer than 25', 'tasmax', 25, True, 10),\n",
    "    ThresholdDays('Days warmer than than 95th pctle yearlong', 'tasmax', hot_95y, True, 10),\n",
    "    Annual_val('Hottest annual temp', 'tasmax', 'max', 35),\n",
    "    Annual_val('Highest daily precip', 'pr', 'max', 500),\n",
    "]\n",
    "future_years = [(2020, 2030), (2030, 2040), (2040, 2050)]\n",
    "scenario = 'ssp585'\n",
    "results = []\n",
    "for hazard in HAZARDS:\n",
    "    for years in future_years:\n",
    "        start_year, end_year = years\n",
    "        varnames = hazard.varname.split('+')\n",
    "        datasets = getdata(varnames, scenario, lat, lon, start_year, end_year, best_models)\n",
    "        results.append(do_locationhazard(hazard, datasets, (lat, lon), scenario, start_year, end_year, calib_fxns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "12e4b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "oname = 'campinas_newnewnew.csv'\n",
    "with open(oname, 'w') as ofile:\n",
    "    ofile.write('latitude,longitude,hazardname,scenario,model,years,lowEV,meanEV,highEV,lowprob,meanprob,highprob\\n')\n",
    "for res in results:\n",
    "    lat, lon, hazardname, scenario, years, evdict, probdict = res\n",
    "    for modelplus in evdict.keys():\n",
    "        with open(oname, 'a') as ofile:\n",
    "            evvals = evdict[modelplus]\n",
    "            probvals = probdict[modelplus]\n",
    "            ofile.write('{0},{1},{2},{3},{4},{5},{6},{7},{8},{9},{10},{11}\\n'.format(lat, lon, hazardname, scenario, modelplus, years, evvals[0], evvals[1], evvals[2], probvals[0], probvals[1], probvals[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d036fcfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef9124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (halfdegenv)",
   "language": "python",
   "name": "halfdegenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
