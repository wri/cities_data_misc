{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28143f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theodore.wong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\theodore.wong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.4SP5SUA7CBGXUEOC35YP2ASOICYYEQZZ.gfortran-win_amd64.dll\n",
      "C:\\Users\\theodore.wong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "C:\\Users\\theodore.wong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "%matplotlib inline\n",
    "import math, datetime\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import datetime, calendar\n",
    "\n",
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "\n",
    "#ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a24d1106",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INFO = {'UKESM1-0-LL': 'HadAM',\n",
    " 'NorESM2-MM': 'CCM',\n",
    " 'NorESM2-LM': 'CCM',\n",
    " 'MRI-ESM2-0': 'UCLA GCM',\n",
    " 'MPI-ESM1-2-LR': 'ECMWF',\n",
    " 'MPI-ESM1-2-HR': 'ECMWF',\n",
    " 'MIROC6': 'MIROC',\n",
    " 'MIROC-ES2L': 'MIROC',\n",
    " 'KIOST-ESM': 'GFDL',\n",
    " 'KACE-1-0-G': 'HadAM',\n",
    " 'IPSL-CM6A-LR': 'IPSL',\n",
    " 'INM-CM5-0': 'INM',\n",
    " 'INM-CM4-8': 'INM',\n",
    " 'HadGEM3-GC31-MM': 'HadAM',\n",
    " 'HadGEM3-GC31-LL': 'HadAM',\n",
    " 'GFDL-ESM4': 'GFDL',\n",
    " 'GFDL-CM4_gr2': 'GFDL',\n",
    " 'GFDL-CM4': 'GFDL',\n",
    " 'FGOALS-g3': 'CCM',\n",
    " 'EC-Earth3-Veg-LR': 'ECMWF',\n",
    " 'EC-Earth3': 'ECMWF',\n",
    " 'CanESM5': 'CanAM',\n",
    " 'CNRM-ESM2-1': 'ECMWF',\n",
    " 'CNRM-CM6-1': 'ECMWF',\n",
    " 'CMCC-ESM2': 'CCM',\n",
    " 'CMCC-CM2-SR5': 'CCM',\n",
    " 'BCC-CSM2-MR': 'CCM',\n",
    " 'ACCESS-ESM1-5': 'HadAM',\n",
    " 'ACCESS-CM2': 'HadAM',\n",
    " 'TaiESM1': 'CCM',\n",
    "}\n",
    "\n",
    "EXCLUDED_MODELS = ['GFDL-CM4_gr2','ERA5'] \n",
    "\n",
    "MODELS = [i for i in MODEL_INFO if not i in EXCLUDED_MODELS]\n",
    "\n",
    "\n",
    "HIST_START = 1980\n",
    "HIST_END = 2014\n",
    "\n",
    "NUM_BEST_MODELS = 5\n",
    "\n",
    "flip_year = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78afd4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = {\n",
    "    'tas': {\n",
    "        'era_name': 'mean_2m_air_temperature',\n",
    "        'era_transform': lambda x: x - 273.15,\n",
    "        'nex_transform': lambda x: x - 273.15\n",
    "    },\n",
    "    'tasmax': {\n",
    "        'era_name': 'maximum_2m_air_temperature',\n",
    "        'era_transform': lambda x: x - 273.15,\n",
    "        'nex_transform': lambda x: x - 273.15\n",
    "    },\n",
    "    'tasmin': {\n",
    "        'era_name': 'minimum_2m_air_temperature',\n",
    "        'era_transform': lambda x: x - 273.15,\n",
    "        'nex_transform': lambda x: x - 273.15\n",
    "    },\n",
    "    'pr': {\n",
    "        'era_name': 'total_precipitation',\n",
    "        'era_transform': lambda x: x * 1000,\n",
    "        'nex_transform': lambda x: x * 86400\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e841055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2j(datestring):\n",
    "    d = datetime.date.fromisoformat(datestring)\n",
    "    jday = d.timetuple().tm_yday\n",
    "    if calendar.isleap(d.year) and jday > 59:\n",
    "        jday -= 1\n",
    "    return jday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1564b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_var(varname, model, latlon, start_year=HIST_START, end_year=HIST_END, yearshift=False, scenario='ssp585'):\n",
    "    def removeLeapDays(arr, yearshift=False):\n",
    "        if not yearshift:\n",
    "            indices = []\n",
    "            jan1_idx = 0\n",
    "            for year in range(start_year, end_year+1):\n",
    "                indices += [jan1_idx + i for i in range(365)]\n",
    "                jan1_idx += 365\n",
    "                if calendar.isleap(year):\n",
    "                    jan1_idx += 1\n",
    "            return arr[indices]\n",
    "        else:\n",
    "            indices = []\n",
    "            jul1_idx = 0\n",
    "            for year in range(start_year-1, end_year):\n",
    "                indices += [jul1_idx + i for i in range(183)]\n",
    "                jul1_idx += 183\n",
    "                if calendar.isleap(year):\n",
    "                    jul1_idx += 1\n",
    "                indices += [jul1_idx + i for i in range(182)]\n",
    "                jul1_idx += 182\n",
    "            return arr[indices]\n",
    "    if model != 'ERA5' and start_year < 2015 and end_year >= 2015:\n",
    "        raise Exception(\"Requesting hist and non-hist variables in one query\")\n",
    "    if model == 'ERA5':\n",
    "        dataset = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\n",
    "    else:\n",
    "        dataset = ee.ImageCollection('NASA/GDDP-CMIP6').filter(ee.Filter.eq('model', model)).filter(ee.Filter.eq('scenario', [scenario, 'historical'][int(end_year<2015)]))\n",
    "    gee_geom = ee.Geometry.Point((latlon[1], latlon[0]))\n",
    "    select_varname = [varname, VARIABLES[varname]['era_name']][int(model == 'ERA5')]\n",
    "    if not yearshift:\n",
    "        data_vars = dataset.select(select_varname).filter(ee.Filter.date('{0}-01-01'.format(start_year), '{0}-01-01'.format(end_year+ 1)))\n",
    "        result = [i[4] for i in data_vars.getRegion(gee_geom, 2500, 'epsg:4326').getInfo()[1:]]\n",
    "        return [VARIABLES[varname]['nex_transform'], VARIABLES[varname]['era_transform']][int(model == 'ERA5')](removeLeapDays(np.array(result), False))\n",
    "    else:\n",
    "        data_vars = dataset.select(select_varname).filter(ee.Filter.date('{0}-07-01'.format(start_year-1), '{0}-07-01'.format(end_year)))\n",
    "        result = [i[4] for i in data_vars.getRegion(gee_geom, 2500, 'epsg:4326').getInfo()[1:]]\n",
    "        return [VARIABLES[varname]['nex_transform'], VARIABLES[varname]['era_transform']][int(model == 'ERA5')](removeLeapDays(np.array(result), True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "015e7fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmsd(d1, d2):\n",
    "    def seasonal_means(d):\n",
    "        mam = []  # 60-151\n",
    "        jja = []  # 152-243\n",
    "        son = []  # 244-334\n",
    "        djf = []  # 335-59\n",
    "        jan1_idx = 365 + [0, 1][int(calendar.isleap(HIST_START))]\n",
    "        for year in range(HIST_START+1, HIST_END):\n",
    "            mam.append(d[jan1_idx + 60 : jan1_idx + 152])\n",
    "            jja.append(d[jan1_idx + 152 : jan1_idx + 244])\n",
    "            son.append(d[jan1_idx + 244 : jan1_idx + 335])\n",
    "            if year < HIST_END - 1:\n",
    "                if False and calendar.isleap(year):\n",
    "                    yearlength = 366\n",
    "                else:\n",
    "                    yearlength = 365\n",
    "                djf.append(np.concatenate((d[jan1_idx + 335 : jan1_idx + 365], d[jan1_idx + yearlength : jan1_idx + yearlength + 60])))\n",
    "            else:\n",
    "                djf.append(np.concatenate((d[335 : 365], d[365 + [0, 1][int(False and calendar.isleap(HIST_START))] : 425])))\n",
    "            jan1_idx += 365 + [0, 1][int(False and calendar.isleap(year))]\n",
    "        return np.array([np.mean(mam, axis=1), np.mean(jja, axis=1), np.mean(son, axis=1), np.mean(djf, axis=1)]).flatten()\n",
    "            \n",
    "    c1 = seasonal_means(d1)\n",
    "    c2 = seasonal_means(d2)\n",
    "    return np.sqrt(np.mean(np.sum((c1 - c2)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e7a7fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quarters(d, start_year, end_year):\n",
    "    mam = []  # 60-151\n",
    "    jja = []  # 152-243\n",
    "    son = []  # 244-334\n",
    "    djf = []  # 335-59\n",
    "    jan1_idx = 365# + [0, 1][int(calendar.isleap(start_year))]\n",
    "    for year in range(start_year, end_year):\n",
    "        tmp = np.concatenate((d[jan1_idx - 365 : jan1_idx - 365 + 60], d[jan1_idx + 335 : jan1_idx + 365]), axis=0)\n",
    "        djf.append(tmp)\n",
    "        mam.append(d[jan1_idx + 60 : jan1_idx + 152])\n",
    "        jja.append(d[jan1_idx + 152 : jan1_idx + 244])\n",
    "        son.append(d[jan1_idx + 244 : jan1_idx + 335])\n",
    "\n",
    "        jan1_idx += 365 + [0, 0][int(False and calendar.isleap(year))]\n",
    "    mam_res = np.vstack(mam)\n",
    "    jja_res = np.vstack(jja)\n",
    "    son_res = np.vstack(son)\n",
    "    djf_res = np.vstack(djf)\n",
    "    return mam_res, jja_res, son_res, djf_res\n",
    "    \n",
    "def seasonal_means(d):\n",
    "    q = quarters(d, HIST_START, HIST_END)\n",
    "    return np.array([np.mean(q[0], axis=1), np.mean(q[1], axis=1), np.mean(q[2], axis=1), np.mean(q[3], axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af2d7c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration_function(hist_obs, hist_mod):\n",
    "    source = np.sort(hist_obs.flatten())\n",
    "    target= np.sort(hist_mod.flatten())\n",
    "   \n",
    "    if (np.max(source) == 0 and np.min(source) == 0):\n",
    "        return np.arange(0, target.size) / target.size\n",
    "    if (np.max(target) == 0 and np.min(target) == 0):\n",
    "        return np.arange(0, source.size) / source.size\n",
    "    new_indices = []\n",
    "    #source[-1] = target[-1]  # when target[i] greater than all source values, return max index\n",
    "    for target_idx, target_value in enumerate(target):\n",
    "        if target_idx < len(source):\n",
    "            source_value = source[target_idx]\n",
    "            if source_value > target[-1]:\n",
    "                new_indices.append(target.size - 1)\n",
    "            else:\n",
    "                new_indices.append(np.argmax(target >= source_value))\n",
    "    return np.array(new_indices) / source.size\n",
    "\n",
    "def calibrate_component(uncalibrated_data, calibration_fxn):\n",
    "    N = len(uncalibrated_data)\n",
    "    unsorted_uncalib = [(i, idx) for idx, i in enumerate(uncalibrated_data)]\n",
    "    sorted_uncalib = sorted(unsorted_uncalib)\n",
    "    result = [0] * N\n",
    "    for j in range(N):\n",
    "        X_j = j / (N + 1)\n",
    "        Y_jprime = calibration_fxn[math.floor(X_j * len(calibration_fxn))]\n",
    "        jprime = math.floor(Y_jprime * (N + 1))\n",
    "        result[sorted_uncalib[j][1]] = sorted_uncalib[min(len(sorted_uncalib)-1, jprime)][0]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def calibrate(uncalibrated_data, calibration_fxn):\n",
    "    mam = []\n",
    "    jja = []\n",
    "    son = []\n",
    "    djf = []\n",
    "    mam_idx = []\n",
    "    jja_idx = []\n",
    "    son_idx = []\n",
    "    djf_idx = []\n",
    "    for idx, i in enumerate(uncalibrated_data):\n",
    "        if idx % 365 >= 60 and idx % 365 < 152:\n",
    "            mam.append(uncalibrated_data[idx])\n",
    "            mam_idx.append(idx)\n",
    "        elif idx % 365 >= 152 and idx % 365 < 244:\n",
    "            jja.append(uncalibrated_data[idx])\n",
    "            jja_idx.append(idx)\n",
    "        elif idx % 365 >= 244 and idx % 365 < 335:\n",
    "            son.append(uncalibrated_data[idx])\n",
    "            son_idx.append(idx)\n",
    "        else:\n",
    "            djf.append(uncalibrated_data[idx])\n",
    "            djf_idx.append(idx)\n",
    "    \n",
    "    mam_calib = calibrate_component(np.array(mam), calibration_fxn[0])\n",
    "    jja_calib = calibrate_component(np.array(jja), calibration_fxn[1])\n",
    "    son_calib = calibrate_component(np.array(son), calibration_fxn[2])\n",
    "    djf_calib = calibrate_component(np.array(djf), calibration_fxn[3])\n",
    "    \n",
    "    result = [0] * len(uncalibrated_data)\n",
    "    for i in range(len(mam_idx)):\n",
    "        result[mam_idx[i]] = mam_calib[i]\n",
    "    for i in range(len(jja_idx)):\n",
    "        result[jja_idx[i]] = jja_calib[i]\n",
    "    for i in range(len(son_idx)):\n",
    "        result[son_idx[i]] = son_calib[i]\n",
    "    for i in range(len(djf_idx)):\n",
    "        result[djf_idx[i]] = djf_calib[i]\n",
    "\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "669ef31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bestmodels(varname, latlon):\n",
    "# Select three best models based on RMSD of quarterly mean tasmax\n",
    "    hist_obs = get_var(varname, 'ERA5', latlon)\n",
    "    hist_mods = {}\n",
    "    rmsds = []\n",
    "    for model in MODELS:\n",
    "        hist_mod = get_var(varname, model, latlon, start_year=HIST_START-1)\n",
    "        hist_mods[model] = hist_mod\n",
    "        rmsds.append((get_rmsd(hist_obs, hist_mod), model))\n",
    "    rmsds.sort()\n",
    "    best_models = []\n",
    "    families = []\n",
    "    idx = 0\n",
    "    while len(best_models) < NUM_BEST_MODELS:\n",
    "        if not MODEL_INFO[rmsds[idx][1]] in families:\n",
    "            best_models.append(rmsds[idx][1])\n",
    "            families.append(MODEL_INFO[rmsds[idx][1]])\n",
    "        idx += 1\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b57acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "latlons = {}\n",
    "with open('latlon_Tamilnadu_Andhra.csv', 'r') as ifile:\n",
    "    lines = ifile.readlines()\n",
    "for line in lines[1:]:\n",
    "    items = line.split(',')\n",
    "    latlons[int(items[0])] = (float(items[1]), float(items[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b26f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def d2j(datestring):\n",
    "    # Date to Julian date\n",
    "    d = datetime.date.fromisoformat(datestring)\n",
    "    jday = d.timetuple().tm_yday\n",
    "    if calendar.isleap(d.year) and jday > 59:\n",
    "        jday -= 1\n",
    "    return jday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "63316e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_latlons = gpd.GeoDataFrame({'locnum': list(latlons.keys()), 'geometry': [shapely.Point(i[1], i[0]) for i in latlons.values()]})\n",
    "centroid_point = all_latlons.dissolve().centroid\n",
    "centroid_latlon = (centroid_point.y[0], centroid_point.x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1ac7aba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 8.25 s\n",
      "Wall time: 20min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_models = {varname: get_bestmodels(varname, centroid_latlon) for varname in ['tas', 'tasmax', 'tasmin', 'pr']}\n",
    "\n",
    "with open('bestmodels.csv', 'a') as ofile:\n",
    "    for varname in best_models:\n",
    "        ofile.write('{0},{1},{2},{3},{4},{5}\\n'.format(varname, *best_models[varname]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "14e56bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_fxns = {}\n",
    "for varname in ['tas', 'tasmax', 'tasmin', 'pr']:\n",
    "    calibration_fxns[varname] = {}\n",
    "    for model in best_models[varname]:\n",
    "        hist_mod = get_var(varname, model, centroid_latlon, yearshift = flip_year)\n",
    "        hist_obs = get_var(varname, 'ERA5', centroid_latlon)\n",
    "        o_quarters = quarters(hist_obs, HIST_START, HIST_END)\n",
    "        m_quarters = quarters(hist_mod, HIST_START, HIST_END)\n",
    "        calibration_fxns[varname][model] = [calibration_function(o_quarters[i].flatten(), m_quarters[i].flatten()) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01609e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('calib_fxns.csv', 'w') as ofile:\n",
    "    for varname in calibration_fxns:\n",
    "        for model in calibration_fxns[varname]:\n",
    "            for season in range(4):\n",
    "                ofile.write('{0},{1},{2},{3}\\n'.format(varname, model, season+1, ','.join([str(i) for i in list(calibration_fxns[varname][model][season])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f13f6ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hazard:\n",
    "    def get_percentile(self, latlon, q):\n",
    "        southern_hem = int(latlon[0] < 0)\n",
    "        era_data = {}\n",
    "        scenario_years = {}\n",
    "        varnames = self.varname.split('+')\n",
    "        for varname in varnames:\n",
    "            era_data[varname] = get_observed_gee(varname, latlon, PERCENTILE_STARTYEAR, PERCENTILE_ENDYEAR, southern_hem)\n",
    "        countdist = self.val_dist([ed for ed in era_data])\n",
    "        return np.percentile(countdist, q)\n",
    "    \n",
    "    def get_expectedval(self, latlon, dataset, calib_fxns, start_year, end_year):\n",
    "        # Take uncalibrated data, calibrate it, apply val_dist() to calibrated data, use resulting freq dist\n",
    "        # to parameterize Dirichlet prior, take resulting vector to parameterize multinomial distribution,\n",
    "        # sample from that multinomial to generate predictive distribution of freq distributions, and return statistics\n",
    "        # (mean and stdev but it could be anything) of the predictive distribution.\n",
    "        \n",
    "        southern_hem = int(latlon[0] < 0)\n",
    "        \n",
    "        numbins = end_year - start_year + 1\n",
    "        \n",
    "        calib_data = np.array(calibrate(dataset, calib_fxns))\n",
    "        countdist = self.val_dist([calib_data[[0,152][int(not southern_hem)]:[len(calib_data),-213][int(not southern_hem)]]])\n",
    "        if countdist is None:\n",
    "            para_res[modelplus] = None\n",
    "        else:\n",
    "            observed_vals = np.array(list(countdist.keys()))\n",
    "            cdist = {}\n",
    "            minval = observed_vals[0]\n",
    "            maxval = observed_vals[-1]\n",
    "            D = (maxval - minval) / (numbins - 1)\n",
    "            for i in range(numbins):\n",
    "                centerval = minval + (i * D)\n",
    "                cdist[centerval] = 0\n",
    "            for count in countdist:\n",
    "                for centerval in cdist:\n",
    "                    if count >= centerval - (D/2) and count < centerval + (D/2):\n",
    "                        cdist[centerval] += 1\n",
    "            alpha = np.array(list(cdist.values())) + (1/numbins)\n",
    "            res = []\n",
    "            for i in range(10000):\n",
    "                dirich_samp = np.random.dirichlet(alpha, 1)\n",
    "                mult_samp = np.random.multinomial(end_year - start_year + 1, dirich_samp[0], 1)[0]\n",
    "                res.append(sum([list(cdist.keys())[j] * mult_samp[j] for j in range(len(list(cdist.keys())))]) / (end_year - start_year + 1))\n",
    "            res = np.array(res)\n",
    "\n",
    "        result = {}\n",
    "        if res is None:\n",
    "            result = [-9999, -9999, -9999]\n",
    "        else:\n",
    "            result = [np.mean(res), np.std(res), -9999]  # -9999 is dummy to preserve format from earier version\n",
    "        return result\n",
    "    \n",
    "\n",
    "class ThresholdDays(Hazard):\n",
    "    # Num days (can be nonconsecutive) meeting some criterion in a year\n",
    "    def __init__(self, hazname, varname, var_threshold, want_max):\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.var_threshold = var_threshold\n",
    "        self.want_max = want_max\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')   \n",
    "        byyear = data.reshape(data.size // 365, 365)\n",
    "        \n",
    "        if self.want_max:\n",
    "            vals = np.sum(byyear >= self.var_threshold, axis=1)\n",
    "        else:\n",
    "            vals = np.sum(byyear <= self.var_threshold, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "    \n",
    "class ThresholdDaysSeasonal(Hazard):\n",
    "    # Num days (can be nonconsecutive) meeting some criterion in a year\n",
    "    def __init__(self, hazname, varname, var_threshold, want_max, startdate, enddate):\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.var_threshold = var_threshold\n",
    "        self.want_max = want_max\n",
    "        self.startdate = startdate\n",
    "        self.enddate = enddate\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "\n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        if data.size % 365 != 0:\n",
    "            raise Exception('Data array length is not an integer multiple of 365')   \n",
    "        byyear = data.reshape(data.size // 365, 365)\n",
    "        start_jday = d2j('1999-{0}'.format(self.startdate)) - [0, 182][int(self.southern_hem)]\n",
    "        end_jday = d2j('1999-{0}'.format(self.enddate)) - [0, 182][int(self.southern_hem)]\n",
    "        if end_jday < start_jday:\n",
    "            end_jday += 365\n",
    "        inseason_onerow = [((i >= start_jday)and(i <= end_jday)) for i in range(365)]\n",
    "        inseason = np.array([inseason_onerow]*(data.size//365))\n",
    "        if self.want_max:\n",
    "            vals = np.sum(inseason >= self.var_threshold, axis=1)\n",
    "        else:\n",
    "            vals = np.sum(inseason <= self.var_threshold, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "    \n",
    "\n",
    "class AnnualVal(Hazard):\n",
    "    def __init__(self, hazname, varname, aggtype):\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.aggtype = aggtype\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        \n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        byyear = data.reshape(data.size//365, 365)\n",
    "        if self.aggtype == 'sum':\n",
    "            vals = np.sum(byyear, axis=1)\n",
    "        elif self.aggtype == 'mean':\n",
    "            vals = np.mean(byyear, axis=1)\n",
    "        elif self.aggtype == 'max':\n",
    "            vals = np.max(byyear, axis=1)\n",
    "        elif self.aggtype == 'min':\n",
    "            vals = np.min(byyear, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        return result_dist\n",
    "\n",
    "class SeasonalVal(Hazard):\n",
    "    # Sum, mean, max, or min of some variable value within a year\n",
    "    def __init__(self, hazname, varname, aggtype, startdate, enddate, southern_hem):\n",
    "        self.hazname = hazname\n",
    "        self.varname = varname\n",
    "        self.aggtype = aggtype\n",
    "        self.startdate = startdate\n",
    "        self.enddate = enddate\n",
    "        self.southern_hem = southern_hem\n",
    "        self.probmodel = 'binomial'\n",
    "        self.exceed_is_gte = True\n",
    "        \n",
    "    def val_dist(self, datalist):\n",
    "        data = datalist[0]\n",
    "        byyear = data.reshape(data.size//365, 365)\n",
    "        start_jday = d2j('1999-{0}'.format(self.startdate)) - [0, 182][int(self.southern_hem)]\n",
    "        end_jday = d2j('1999-{0}'.format(self.enddate)) - [0, 182][int(self.southern_hem)]\n",
    "        if end_jday < start_jday:\n",
    "            end_jday += 365\n",
    "        inseason_onerow = [((i >= start_jday)and(i <= end_jday)) for i in range(365)]\n",
    "        inseason = np.array([inseason_onerow]*(data.size//365))\n",
    "        byyear = byyear * inseason\n",
    "        if self.aggtype == 'sum':\n",
    "            vals = np.sum(byyear, axis=1)\n",
    "        elif self.aggtype == 'mean':\n",
    "            vals = np.mean(byyear, axis=1)\n",
    "        elif self.aggtype == 'max':\n",
    "            vals = np.max(byyear, axis=1)\n",
    "        elif self.aggtype == 'min':\n",
    "            vals = np.min(byyear, axis=1)\n",
    "        result_dist = {}\n",
    "        for val in np.unique(vals):\n",
    "            result_dist[val] = np.sum(vals == val)\n",
    "        #print(vals)\n",
    "        return result_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "bc558cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEASONS = {\n",
    "    'SW-monsoon': {'start': '06-01', 'end': '09-30'},\n",
    "    'NE-monsoon': {'start': '11-01', 'end': '12-31'}\n",
    "}\n",
    "\n",
    "FUTURE_INTERVALS = [\n",
    "    {'start': 1995, 'end': 2004},\n",
    "    {'start': 2005, 'end': 2014},\n",
    "    {'start': 2020, 'end': 2029},\n",
    "    {'start': 2030, 'end': 2039},\n",
    "    {'start': 2040, 'end': 2049},\n",
    "    {'start': 2050, 'end': 2059},\n",
    "    {'start': 2060, 'end': 2069},\n",
    "    {'start': 2070, 'end': 2079},\n",
    "    {'start': 2080, 'end': 2089},\n",
    "    {'start': 2090, 'end': 2099}\n",
    "]\n",
    "\n",
    "SCENARIOS = ['ssp245', 'ssp585']\n",
    "\n",
    "HAZARDS = [\n",
    "    SeasonalVal('mean maxtemp SWMonsoon', 'tasmax', 'mean', SEASONS['SW-monsoon']['start'], SEASONS['SW-monsoon']['end'], False), # Mean seasonal Maximum temperature\n",
    "    SeasonalVal('mean maxtemp NEMonsoon', 'tasmax', 'mean', SEASONS['NE-monsoon']['start'], SEASONS['NE-monsoon']['end'], False),\n",
    "    SeasonalVal('mean mintemp SWMonsoon', 'tasmin', 'mean', SEASONS['SW-monsoon']['start'], SEASONS['SW-monsoon']['end'], False), # Mean seasonal Minimum temperature\n",
    "    SeasonalVal('mean mintemp NEMonsoon', 'tasmin', 'mean', SEASONS['NE-monsoon']['start'], SEASONS['NE-monsoon']['end'], False),\n",
    "    SeasonalVal('total precip SWMonsoon', 'pr', 'mean', SEASONS['SW-monsoon']['start'], SEASONS['SW-monsoon']['end'], False), # Seasonal Cumulative Rainfall\n",
    "    SeasonalVal('total precip NEMonsoon', 'pr', 'mean', SEASONS['NE-monsoon']['start'], SEASONS['NE-monsoon']['end'], False),\n",
    "    ThresholdDaysSeasonal('days precip-gte-0 SWMonsoon', 'pr', 0.001, True, SEASONS['SW-monsoon']['start'], SEASONS['SW-monsoon']['end']), # Seasonal number of rainy days\n",
    "    ThresholdDaysSeasonal('days precip-gte-0 NEMonsoon', 'pr', 0.001, True, SEASONS['NE-monsoon']['start'], SEASONS['NE-monsoon']['end']),\n",
    "    AnnualVal('mean meantemp', 'tas', 'mean'), # Projected Change in Annual Average Temperature\n",
    "    ThresholdDays('days precip-gte-10', 'pr', 10, True), # Projected Change in Extreme Precipitation Days\n",
    "    AnnualVal('mean mintemp', 'tasmin', 'mean'), # Projected Change in Annual Average Minimum Temperature\n",
    "    AnnualVal('mean maxtemp', 'tasmax', 'mean'), # Projected Change in Annual Average Maximum Temperature\n",
    "    AnnualVal('total precip', 'pr', 'sum'), # Projected Change in Cumulative Precipitation\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9c677dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lats = []\n",
    "total_lons = []\n",
    "with open('total_latlons.csv', 'r') as ifile:\n",
    "    lines = ifile.readlines()\n",
    "for line in lines:\n",
    "    items = [i.strip() for i in line.split(',')]\n",
    "    if items[0]:\n",
    "        total_lats.append(float(items[0]))\n",
    "    if items[1]:\n",
    "        total_lons.append(float(items[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "96aff9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_locationhazard(hazard, dataset, calib_fxns, start_year, end_year):\n",
    "    return hazard.get_expectedval(latlon, dataset, calib_fxns, start_year, end_year)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5880c849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean maxtemp SWMonsoon\n",
      "ssp245\n",
      "  1995-2004\n",
      "1 2 3 4 5 "
     ]
    }
   ],
   "source": [
    "for hazard in HAZARDS:\n",
    "    print(hazard.hazname)\n",
    "    for scenario_idx, scenario in enumerate(SCENARIOS):\n",
    "        print(scenario)\n",
    "        for future_interval in FUTURE_INTERVALS:\n",
    "            if future_interval['end'] >= 2015 or scenario_idx == 0:\n",
    "                print(\"  {0}-{1}\".format(future_interval['start'], future_interval['end']))\n",
    "                num_done = 0\n",
    "                means = []\n",
    "                stds = []\n",
    "                for lat_idx, lat in enumerate(total_lats):\n",
    "                    for lon_idx, lon in enumerate(total_lons):\n",
    "                        latlon = (lat, lon)\n",
    "                        if latlon in list(latlons.values()):\n",
    "                            result = []\n",
    "                            for model in best_models[hazard.varname]:\n",
    "                                dataset = get_var(hazard.varname, model, latlon, start_year=future_interval['start'], end_year=future_interval['end'], yearshift=False, scenario=scenario)\n",
    "                                calib_fxns = calibration_fxns[hazard.varname][model]\n",
    "                                result.append(do_locationhazard(hazard, dataset, calib_fxns, start_year=future_interval['start'], end_year=future_interval['end']))\n",
    "                            result = np.array(result)\n",
    "                            means.append(result.mean())\n",
    "                            stds.append(result.std())\n",
    "                            num_done += 1\n",
    "                            print(num_done, end=' ')\n",
    "                        else:\n",
    "                            means.append(None)\n",
    "                            stds.append(None)\n",
    "                print()\n",
    "                outarray_mean = np.array(means).reshape(len(total_lats), len(total_lons))\n",
    "                outarray_std = np.array(stds).reshape(len(total_lats), len(total_lons))\n",
    "                xarray_mean= xr.DataArray(outarray_mean, {'latitude': total_lats, 'longitude': total_lons})\n",
    "                xarray_std= xr.DataArray(outarray_std, {'latitude': total_lats, 'longitude': total_lons})\n",
    "                xarray_mean.to_raster('{0}__{1}_{2}-{3}_mean.tif'.format(hazard.hazname.replace(' ', '_'), scenario, future_interval['start'], future_interval['end']))\n",
    "                xarray_std.to_raster('{0}__{1}_{2}-{3}_std.tif'.format(hazard.hazname.replace(' ', '_'), scenario, future_interval['start'], future_interval['end']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4b962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
